
@Misc{2018_vid13,
author    = {Guttandin, Christoph},
title     = {The Timing Object - A Pacemaker for the Web},
month     = sep,
year      = {2018},
abstract  = {The web knows many different timing mechanisms which all have their unique use case. Unfortunately there is no easy way to align any of those timers. They often have their own timeline, unit and level of accuracy. How would you for example synchronize a slide show on a beamer with the audio recording of the speaker played back on a different device? The Timing Object is aiming to solve that problem. It is a W3C Draft which introduces yet another timing mechanism. But it is completely unopinionated about what it controls. It is specifically designed to control multiple timed sources in sync. Sadly the Timing Object hasn't gained much traction so far. No browser vendor has implemented it yet or is currently planning to do so. The specification process itself seems to be slowing down as well. But that doesn't have to be the end of the story. The Timing Object can mostly be implemented in user land and some parts of the spec are purposefully incomplete to allow different implementations by third party vendors. I want to show what is already possible and how everybody can start using the Timing Object right now in their applications. As many of the presentations and demos at previous WACs have shown, many people have built their own custom solution to realize distributed synchronization. I hope to raise interest in the Timing Object Draft and motivate more people to contribute their experience to the standardization process. One of the things which is meant to be implemented by third party vendors is the TimingProvider. The TimingProvider is responsible for synchronizing Timing Objects across different devices. I want to demonstrate the usage of a TimingProvider which uses WebRTC internally to setup the communication between participating devices. There are of course also some parts of the specification which have to be build by browser vendors. But I'm sure that if the Timing Object gets used in the wild the browser vendors will eventually start to implement it natively.},
address   = {Berlin},
booktitle = {Proceedings of the International Web Audio Conference},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/d50cc9562f6d6511a3d1d0360bf0efd769648310.html:html},
publisher = {TU Berlin},
series    = {WAC '18},
type      = {Video},
url       = {https://www.youtube.com/watch?v=0xqVzTIVEB4},
}

@Misc{2018_vid12,
author    = {Stolfi, Ariane and Milo, Alessia and Barthet, Mathieu},
title     = {Tender Buttons | Sound | Space},
month     = sep,
year      = {2018},
abstract  = {In this performance, we propose an interpretation of Gertrude Steins' poem Tender Buttons, using Playsound.space participatory player to create a soundscape generated live from the text contents. Two performers will dialogue through the chat interface with excerpts of the text while a reading of the poem runs on the background. While the text is render, searches on the system will be made, to access Freesound.org Creative Commons content to create a music improvisation based on the piece.},
address   = {Berlin},
booktitle = {Proceedings of the International Web Audio Conference},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/152d18212d7d92621c35b0a434c5d9beec09ec85.html:html},
publisher = {TU Berlin},
series    = {WAC '18},
type      = {Video},
url       = {https://www.youtube.com/watch?v=mFlITzqRBWY},
}

@Misc{2018_vid8,
author    = {Henderson, Walker},
title     = {Latency and Synchronization in Web Audio},
month     = sep,
year      = {2018},
abstract  = {The recording and playback of audio on a computer is always subject to latency and Web Audio applications are no different. Latency makes it difficult to write software that properly synchronizes computer audio with “real world” audio and interactions. In many situations, Web Audio apps have to deal with huge amounts of latency. Humans have issues with timing too: it's often hard for us to perform actions perfectly on rhythm or in synchrony with our computer's understanding of time. In this talk, I'll detail the techniques I used to develop a live-looping app using Web Audio, including methods for measuring and handling various types of latency and structuring code to avoid spreading latency-handling logic throughout the application. I'll also discuss features that help humans more easily express timing in music, like allowing a loop to start before the “record” button was pressed and implementing button press “fudging” that starts, stops, and records loops at the time the performer meant to press the button, not the time they actually pressed the button.},
address   = {Berlin},
booktitle = {Proceedings of the International Web Audio Conference},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
publisher = {TU Berlin},
series    = {WAC '18},
type      = {Video},
url       = {https://www.youtube.com/watch?v=8-rR1y8zu3M},
}

@Misc{2018_vid14,
author    = {Violi, Nicholas},
title     = {WebSonify: Ambient aural display of real-time data},
month     = sep,
year      = {2018},
abstract  = {Data visualization as a field has made tremendous progress in exploring ways to investigate, interrogate, and comprehend abstract data by visual means. For all its benefits, data visualization has some drawbacks when it comes to live, streaming data. Some tools have attempted to solve this problem using sonification -- translating live data to sound or noise -- but these devices traditionally have not attempted to produce sound that is interesting, varying, or aesthetically pleasing. Nick will present a tool he created in which he connected a website's live user data to the Web Audio API to create a procedurally-generated and (theoretically) infinite "song" representing the site's traffic patterns. He will reflect on the potential for using sound and music as a medium for developing an intuitive understanding of streaming data.},
address   = {Berlin},
booktitle = {Proceedings of the International Web Audio Conference},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
publisher = {TU Berlin},
series    = {WAC '18},
type      = {Video},
url       = {https://www.youtube.com/watch?v=Suwx85JFZBQ},
}

@Misc{2018_vid9,
author    = {Guffond, Jasmine and Eriksen, Jacob},
title     = {Listening Back - The Web Never Forgets},
month     = sep,
year      = {2018},
abstract  = {A live performance by the Browser Duo (Jasmine Guffond & Jacob Eriksen), will explore the potential of the ‘Listening Back' browser plug-in as a musical instrument. ‘Listening Back' is a plug-in for the Chrome Browser that sonifies internet cookies in real time as one browses online. Utilising digital waveform synthesis, ‘Listening Back' translates internet cookies into sound thereby creating an audible presence for hidden infrastructures that collect personal and identifying data by storing a file on one's computer. Through the direct intervention of the World Wide Web as a technological, social and political platform, web audio is employed to expose the proliferation of ubiquitous online surveillance. Our access to the Web is visually mediated through screen devices and this project explores how sound can help us engage with complex phenomena beyond the graphical interface of the world wide web.},
address   = {Berlin},
booktitle = {Proceedings of the International Web Audio Conference},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
publisher = {TU Berlin},
series    = {WAC '18},
type      = {Video},
url       = {https://www.youtube.com/watch?v=f9a7kWrngiA},
}

@Misc{2018_vid10,
author    = {Parviainen, Tero},
title     = {Musical Deep Neural Networks in the Browser},
month     = sep,
year      = {2018},
abstract  = {2018 is the year deep neural networks have arrived in the web browser. This development has many exciting prospects. One of them is that all the work AI researchers have put into musical applications of neural nets is suddenly available to use in web apps. We can now use generative musical deep learning models and build interactive web-based experiences on top of them, with the help of Web Audio and the rest of the web platform. In this talk I will present some recent experiments I've made using deep neural nets and Web Audio in the browser. They're mostly based on Google's Magenta.js library and neural net models trained by the Magenta team. We'll see how recurrent neural networks can be used to build melodic “autocompletion” tools and arpeggiators, as well as generative drum patterns. We'll also see how to use variational autoencoder models to explore latent musical spaces: Interpolating between melodies, and exploring musical generative space with vector arithmetic.},
address   = {Berlin},
booktitle = {Proceedings of the International Web Audio Conference},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
publisher = {TU Berlin},
series    = {WAC '18},
type      = {Video},
url       = {https://www.youtube.com/watch?v=HKRJuz6o2uY},
}

@Misc{2018_vid6,
author    = {Xambó, Anna},
title     = {Imaginary Berlin},
month     = sep,
year      = {2018},
abstract  = {This piece invites the audience to create a collaborative soundscape based on audio streams from the area of Berlin and by using their mobile devices. Influenced by the collaborative music piece Imaginary Landscape No. 4 (1951) by John Cage, 12 audio streams from Berlin-inspired radio stations will be emitted from a central laptop connected to a PA system. The audience will be able to pick one audio stream at any time by moving horizontally their mobile phones, and control the volume of the selected audio stream by moving vertically their mobile phones. The original Cage's piece was designed for 12 pairs of performers, here the emphasis remains in the group participation and creation of a larger musical network by the potential generation of delays and localized sounds within the same space.},
address   = {Berlin},
booktitle = {Proceedings of the International Web Audio Conference},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
publisher = {TU Berlin},
series    = {WAC '18},
type      = {Video},
url       = {https://www.youtube.com/watch?v=v7FwOEy0jK4},
}

@Misc{2018_vid3,
author    = {Bundin, Andrey},
title     = {Concert for Smartphones and Orchestra},
month     = sep,
year      = {2018},
abstract  = {Violin - Alexey Kotchetkov Viola - Coco Elane Cello - Ladis Cinzek This audience devices participative performance combines symphonic orchestra with the choir of smartphones. Author tried different approaches to synchronisation of acoustic instruments and audience devices: tonal synchronisation with fine tuning of all samples and synthesis engine, latency compensation depending on device type and network latency, tempo calculation e.t.c.. This performance significantly depends on Web Audio API and Web MIDI API.},
address   = {Berlin},
booktitle = {Proceedings of the International Web Audio Conference},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
publisher = {TU Berlin},
series    = {WAC '18},
type      = {Video},
url       = {https://www.youtube.com/watch?v=-_Q6x4mfZAM},
}

@Misc{2018_vid2,
author    = {Pulver, Tim and Kombüchen, Thomas},
title     = {cables—a web based visual programming language for WebGL and Web Audio},
month     = sep,
year      = {2018},
abstract  = {With cables, a new web-based visual programming environment, creating WebGL and Web Audio applications can be done using a visual editor, without writing any code. It allows to create audio-visual experiments in a playful way.},
address   = {Berlin},
booktitle = {Proceedings of the International Web Audio Conference},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
publisher = {TU Berlin},
series    = {WAC '18},
type      = {Video},
url       = {https://www.youtube.com/watch?v=32xHw6BWIRI},
}

@Misc{2018_vid7,
author    = {Roberts, Charles},
title     = {Improvisation by Charles Roberts},
month     = sep,
year      = {2018},
abstract  = {This performance will be the first to use a completely rewritten version of Gibber, taking advantage of music programming techniques found in the live-coding environment gibberwocky, and upgraded versions of multiple audio libraries designed to take advantage of AudioWorklets.},
address   = {Berlin},
booktitle = {Proceedings of the International Web Audio Conference},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
publisher = {TU Berlin},
series    = {WAC '18},
type      = {Video},
url       = {https://www.youtube.com/watch?v=qi8VX6GawLM},
}

@Misc{2018_vid4,
author    = {Ros, Hilke},
title     = {From artist to software developer and back. A celloist's perspective on Web Audio},
month     = sep,
year      = {2018},
abstract  = {During this talk I will elaborate on the interaction between my work as both a musician and a software coder and how my experiences in both areas come together in the production of my first web audio album. Having a background as a classically trained musician and after touring internationally with an indie pop band, I deal with 3 main issues during the process of creating music in the new web audio format. 1. As a coding artist, I try to mould the Web Audio API into actionable music tools. Therefore, I started to translate the Tone.js Framework into a Ruby gem. 2. I try to find a balance between the coding part and my ‘normal' music workflow, i.e. sketching ideas with the intuitive user interface of professional DAW software, in my case: Ableton Live. Therefore I need to find an easy way to transfer musical content between the Ableton and the Web Audio environments. 3. How can I create music with a 'story' and emotional compact in the context of generative and interactive composing techniques?},
address   = {Berlin},
booktitle = {Proceedings of the International Web Audio Conference},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
publisher = {TU Berlin},
series    = {WAC '18},
type      = {Video},
url       = {https://www.youtube.com/watch?v=V3EKXI4QJ-Q},
}

@Misc{2018_KN1,
author    = {John, Ruth},
title     = {The opening keynote for WAC 2018},
month     = sep,
year      = {2018},
abstract  = {Ruth is a web technologist and Google Developer Expert. She likes to educate people about new web technologies and inspire them to try them, coming up with exciting and engaging ways to use them. She's a founding member of Live: JS , a collective of audio and visual artists that solely rely on JavaScript to create their performances. Her favourite things include hacking with hardware, evangelising about Web Audio and MIDI and taking her award nominated, hand coded, audio/visual software to the streets.},
address   = {Berlin},
booktitle = {Proceedings of the International Web Audio Conference},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
publisher = {TU Berlin},
series    = {WAC '18},
type      = {Keynote},
url       = {https://www.youtube.com/watch?v=55ewLshu5xo},
}

@Misc{2018_KN2,
author    = {Rogers, Chris},
title     = {The second keynote of WAC 2018},
month     = sep,
year      = {2018},
abstract  = {Chris Rogers is a software architect, specializing in audio production tools and music applications. Previously he worked at Google, where he developed the Web Audio API and other advanced web platform technologies. At Apple he was a principal designer of the Audio Units plugin architecture, and developed many of the ones shipping on Mac OS X and iOS, including the AUMatrixReverb, and the AUTimePitch time-stretching algorithm. He's also worked for Macromedia, Motorola, Wolfram Research, and IRCAM where he worked on SVP and developed the original AudioSculpt application.},
address   = {Berlin},
booktitle = {Proceedings of the International Web Audio Conference},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
publisher = {TU Berlin},
series    = {WAC '18},
type      = {Keynote},
url       = {https://www.youtube.com/watch?v=iXxxQYOicy8},
}

@Misc{2018_vid5,
author    = {Passing, Lisa},
title     = {Generative music, playful visualizations and where to find them},
month     = sep,
year      = {2018},
abstract  = {Generative music is magical: it writes itself! But it gets even better: With the WebAudio API we can work this magic in the browser, where we also have the possibilities to visualize and interact with it.},
address   = {Berlin},
booktitle = {Proceedings of the International Web Audio Conference},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
publisher = {TU Berlin},
series    = {WAC '18},
type      = {Video},
url       = {https://www.youtube.com/watch?v=vQOtLFDDDS8},
}

@Misc{2018_vid11,
author    = {Roma, Gerard},
title     = {No merge conflicts},
month     = sep,
year      = {2018},
abstract  = {No merge conflicts is a participatory performance based on web audio technologies. Participants use their smartphones to access a synthesizer that produces audiovisual signals following touch and accelerometer gestures. An artificial agent character selects and evolves the audience patterns into an audiovisual algorithmic composition played through the sound system and projector.},
address   = {Berlin},
booktitle = {Proceedings of the International Web Audio Conference},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
publisher = {TU Berlin},
series    = {WAC '18},
type      = {Video},
url       = {https://www.youtube.com/watch?v=n1T5dw71KQI},
}

@Misc{2018_vid1,
author    = {Henri, Mathieu},
title     = {AMBIENT HTML5: Music for tiny airports in 256 bytes},
month     = sep,
year      = {2018},
abstract  = {Forty years ago, Brian Eno released "Ambient 1: Music for Airport". Creating eternally changing music using a simple system. We will see how to use the Web Audio API to pay homage to this milestone release and create hours and hours of music in just 256 bytes.},
address   = {Berlin},
booktitle = {Proceedings of the International Web Audio Conference},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
publisher = {TU Berlin},
series    = {WAC '18},
type      = {Video},
url       = {https://www.youtube.com/watch?v=Lxho0sjXrKY},
}

@InProceedings{2018_6,
author    = {Houge, Ben},
title     = {Cena concertante alla maniera di Vivaldi: Considering the Restaurant as a Musical Interface},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {In recent years, I have been conducting research into the links between music and gastronomy by collaborating with chefs to develop multisensory dining experiences that I call “food operas.” These events incorporate real-time music techniques adapted from the world of video game development to respond to the unpredictable events and timings of the dining room. This paper details an event I developed in collaboration with the Boston Symphony Orchestra, chef David Verdo, and designer Jutta Friedrichs that took place on January 5 and 7, 2017. Cena concertante alla maniera di Vivaldi was a four-course meal with real- time musical accompaniment deployed from a seventy-channel speaker array comprised of sixty-four iPads and six near field studio monitors, all coordinated via the same network. The iPads were positioned in custom-built acoustic resonators placed at each seat in the restaurant, presenting a unique audio channel to each diner, synchronized to the rhythms of each diner's meal and sited as close as possible to the food. The music was based on Vivaldi's Piccolo Concerto in C Major, RV 443, drawing from archival BSO performances, with the objective of enhancing diners' appreciation of a live performance of the work on a concert following the meal. The menu was based on the music, drawing on research in the field of crossmodal psychology that identifies links between the senses of taste and hearing. This paper discusses the background of the project, its musical organization, the infrastructure and control techniques required to execute it, and relevant research in the field of crossmodal psychology, concluding with a discussion of areas for future work.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Houge - 2018 - Cena concertante alla maniera di Vivaldi Considering the Restaurant as a Musical Interface.pdf:pdf},
type      = {Paper},
}

@InProceedings{2018_24,
author    = {Yi, Steven and Lazzarini, Victor and Costello, Ed},
title     = {WebAssembly AudioWorklet Csound},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {This paper describes WebAssembly AudioWorklet (WAAW) Csound, one of the implementations of Web Audio Csound. We begin by introducing the background to this current implementation, stemming from the two first ports of Csound to the web platform using Native Clients and asm.js. The technology of WebAssembly is then introduced and discussed in its more relevant aspects. The AudioWorklet interface of Web Audio API is explored, together with its use in WAAW Csound. We complement this discussion by considering the overarching question of support for multiple plat- forms, which implement different versions of Web Audio. Some initial examples of the system are presented to illus- trate various potential applications. Finally, we complement the paper by discussing current issues that are fundamental for this project and others that rely on the development of a robust support for WASM-based audio computing.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Yi, Lazzarini, Costello - 2018 - WebAssembly AudioWorklet Csound.pdf:pdf},
type      = {Paper},
url       = {https://www.youtube.com/watch?v=OLTBTDOXfjA},
}

@InProceedings{2018_9,
author    = {Matuszewski, Benjamin and Larralde, Joseph and Bevilacqua, Frédéric},
title     = {Designing Movement Driven Audio Applications Using a Web-Based Interactive Machine Learning Toolkit},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
pages     = {2--5},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {This paper presents a web based toolkit for implementing Interactive Machine Learning (IML) dedicated to creative audio applications. The toolkit, composed of a main library and a template application , facilitates the creation of experiences on collective musical interactions with a strong emphasis on real-time movement processing and recognition. At its lower level, the mano-js library proposes a user-friendly API built on top of existing libraries. The library is designed to assist developers and creative coders in the appropriation and usage of the Interactive Machine Learning concepts and workflow, as well as to simplify development of new applications. The library is open-source, based on web standards and released under the BSD-3-Clause Licence. At its higher level, the toolkit proposes Elements, a template application designed towards non-developer users. The application specifically aims at providing a mean for researchers and designers to prototype new movement-based distributed Interactive Machine Learning scenarios. The application allows to create a new scenario by simply providing a JSON configuration file that defines the role and the abilities of each client. The application has been iteratively tested and developed in the context of several workshops.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Matuszewski, Larralde, Bevilacqua - 2018 - Designing Movement Driven Audio Applications Using a Web-Based Interactive Machine Learning T.pdf:pdf},
type      = {Paper},
}

@InProceedings{2018_12,
author    = {Pauwels, Johan and Xambó, Anna and Roma, Gerard and Barthet, Mathieu and Fazekas, György},
title     = {Exploring Real-time Visualisations to Support Chord Learning with a Large Music Collection},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
number    = {Iv},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {A common problem in music education is finding varied and engaging material that is suitable for practising a specific musical concept or technique. At the same time, a number of large music collections are available under a Creative Commons (CC) licence (e.g. Jamendo, ccMixter), but their potential is largely untapped because of the relative obscurity of their content. In this paper, we present Jam with Jamendo, a web application that allows novice and expert learners of musical instruments to query songs by chord content from a large music collection, and practise the chords present in the retrieved songs by playing along. Its goal is twofold: the learners get a larger variety of practice material, while the artists receive increased exposure. We experimented with two visualisation modes. The first is a linear visualisation based on a moving time axis, the second is a circular visualisation inspired by the chromatic circle. We conducted a small-scale thinking-aloud user study with seven participants based on a hands-on practice with the web app. Through this pilot study, we obtained a qualitative understanding of the potentials and challenges of each visualisation, which will be used to inform the next design iteration of the web app.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Pauwels et al. - 2018 - Exploring Real-time Visualisations to Support Chord Learning with a Large Music Collection.pdf:pdf},
type      = {Paper},
url       = {https://www.youtube.com/watch?v=PITvgeIA2pE},
}

@InProceedings{2018_15,
author    = {Roberts, Charles},
title     = {Metaprogramming Strategies for AudioWorklets},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {The introduction of AudioWorklets to the Web Audio API greatly expands the potential of browser-based audio programming. However, managing state between the various threads AudioWorklets occupy entails a fair amount of complexity, particularly when designing dynamic music programming environments where exact digital signal processing requirements cannot be known ahead of time. Such environments are commonly used for live coding performance, interactive composition, and coding playgrounds for musical experimentation. Our research explores metaprogramming strategies to create AudioWorklet implementations for two JavaScript libraries, Genish.js and Gibberish.js. These strategies help hide the complexities of inter-thread communication from end-users and enable a variety of signal processing and interaction techniques that would otherwise be difficult to achieve.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Roberts - 2018 - Metaprogramming Strategies for AudioWorklets.pdf:pdf},
type      = {Paper},
url       = {https://www.youtube.com/watch?v=izXcd4kw0Ds},
}

@InProceedings{2018_2,
author    = {Roma, Gerard and Xambó, Anna and Green, Owen and Tremblay, Pierre Alexandre},
title     = {A Javascript Library for Flexible Visualization of Audio Descriptors},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
pages     = {3--8},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {Research in audio analysis has provided a large number of ways to describe audio recordings, which can be used for enhancing their visual representation in web applications. In this paper we present fav.js, a Javascript library for flexible visualization of audio descriptors. We explain the proposed design and demonstrate its potential for web audio applications through several visualization examples.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Xamb{\'{o}} et al. - 2018 - A Javascript Library for Flexible Visualization of Audio Descriptors.pdf:pdf},
type      = {Paper},
url       = {https://www.youtube.com/watch?v=xwRkXQePIgM},
}

@InProceedings{2018_18,
author    = {Stolfi, Ariane and Milo, Alessia and Ceriani, Miguel and Barthet, Mathieu},
title     = {Participatory musical improvisations with Playsound.space},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {Playsound.space is a web-based tool to search for and play Creative Commons licensed-sounds, which can be applied to free improvisation, experimental music production and soundscape composition. It provides fast access to about 400k non-musical and musical sounds provided by Freesound and allows users to play/loop single or multiple sounds retrieved through text-based search. Sound discovery is facilitated by the use of semantic searches and sound visual representations (spectrograms). After feedback gathered from user tests and practices with the tool as an instrument, we identified several directions to develop the expressive and collaborative capabilities of the tool. We present additional features for more complex audio processing, and also to enhance participation trough a chat system that allows users to share sound sessions and exchange messages while playing.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Stolfi et al. - 2018 - Participatory musical improvisations with Playsound.space.pdf:pdf},
type      = {Paper},
url       = {https://www.youtube.com/watch?v=_djuNZ_gwaw},
}

@InProceedings{2018_5,
author    = {Baumann, Christian and Friederike, Johanna and Milde, Jan-Torsten},
title     = {Body Movement Sonification using the Web Audio API},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
pages     = {5--8},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {In this paper we describe the ongoing research on the development of a body movement sonification system. High precision, high resolution wireless sensors are used to track the body movement and record muscle excitation. We are currently using 6 sensors. In the final version of the system full body tracking can be achieved. The recording system provides a web server including a simple REST API, which streams the recorded data in JSON format. An intermediate proxy server pre-processes the data and transmits it to the final sonification system. The sonification system is implemented using the web audio api. We are experimenting with a set of different sonification strategies and algorithms. Currently we are testing the system as part of an interactive, guided therapy, establishing additional acoustic feedback channels for the patient. In a second stage of the research we are going to use the system in a more musical and artistic way. More specifically we plan to use the system in cooperation with a violist, where the acoustic feedback channel will be integrated into the performance.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Baumann, Friederike, Milde - 2018 - Body Movement Sonification using the Web Audio API.pdf:pdf},
type      = {Paper},
}

@InProceedings{2018_23,
author    = {Cretti, Francesco and Morino, Luca and Liuni, Marco and Gervasoni, Stefano and Agostini, Andrea and Servetti, Antonio},
title     = {Web Wall Whispers: an interactive web-based sound work},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
pages     = {2--3},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {Web Wall Whispers (www) is an interactive sound work that heavily relies on the web audio technology to enable a virtual high-quality multimodal exploration of a monumental mural. The user's navigation through the artwork generates a unique interactive musical composition at every access, in a challenging paradigm of open form based on a virtual dialogue between the visitors and the composer. The project is conceived as a part of the Segni per la Speranza (spls, Signs for Hope) multimodal artwork, a project aimed at the reappraisal of urban outlying areas. All the constituent materials are freely distributed under the open source GNU General Public Licence, thus allowing the build-up of extensions or new versions of this multimodal artwork paradigm.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Cretti et al. - 2018 - Web Wall Whispers an interactive web-based sound work.pdf:pdf},
type      = {Paper},
}

@InProceedings{2018_21,
author    = {Marasco, Anthony T and Allison, Jesse},
title     = {SoundSling: A Framework for Using Creative Motion Data to Pan Audio Across a Mobile Device Speaker Array},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
pages     = {2--4},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {SoundSling is a framework used to translate motion-based data into audio diffusion trajectories across a crowd of networked mobile devices. The intent is to allow a performer to distribute audio across audience mobile devices in creative ways with motion data that mimics various patterns of movement found in the natural world. As a sound is “slung” around the room, the software intuitively adjusts each audience member's gain as it moves past their location. SoundSling adapts dynamically to the total number of devices as users connect to or disconnect from the network. This helps to ensure that the performer's chosen diffusion patterns and motion trajectories can be scaled properly to the array of currently-participating devices. Existing as a collection of MaxMSP abstractions and easily-editable web page templates, a focus has been kept on making the tool as adaptable to a performer's current musical set-up as possible.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Marasco, Allison - 2018 - SoundSling A Framework for Using Creative Motion Data to Pan Audio Across a Mobile Device Speaker Array.pdf:pdf},
type      = {Paper},
}

@InProceedings{2018_8,
author    = {Fyfe, Lawrence and Gladin, Olivier and Fleury, Cédric and Beaudouin-Lafon, Michel},
title     = {Combining Web Audio Streaming, Motion Capture, and Binaural Audio in a Telepresence System},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {This paper describes the use of spatialized binaural 3D audio in the Digiscape telepresence system. Digiscape is a custom-built system that enables groups of users of visualization platforms, including CAVEs and wall-sized displays, to collaborate at a distance while visualizing and manipulating large and complex data sets. Digiscape supports web-based audio and video streaming as well as data sharing among its constituent platforms. Using motion capture systems, the locations of collaborators in each physical spaces are sent to the remote platforms, which spatialize the collaborators' audio streams in their local audio space. The audio spaces can be configured in a variety of ways, from a single shared audio space to mapped, adjacent, contained, split, or distorted audio spaces, facilitating the exploration of the possibilities of spatialized voice communication in telepresence systems.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Fyfe et al. - 2018 - Combining Web Audio Streaming, Motion Capture, and Binaural Audio in a Telepresence System.pdf:pdf},
keywords  = {Opus,RTP,Telepresence,WebRTC,binaural audio,motion capture,remote collaboration},
type      = {Paper},
}

@InProceedings{2018_11,
author    = {Dodds, Thomas},
title     = {dspNode: Real-time remote audio rendering},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {The author has been experimenting with various implementations of real-time cloud based audio rendering, keeping the client side application as an extremely light weight remote controller, receiving the fully rendered audio stream from a cloud based audio rendering engine. The general benefits, drawbacks and conclusions will be discussed with a plausible and functional example application given for the reader's own performance evaluation.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Dodds - 2018 - dspNode Real-time remote audio rendering.pdf:pdf},
type      = {Paper},
url       = {https://www.youtube.com/watch?v=UafQ9Te92p4},
}

@InProceedings{2018_14,
author    = {Mitchusson, Chase and Allison, Jesse},
title     = {Lost In Space: Indoor Localization for Virtual Environment Exploration},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {Lost In Space is a project utilizing Bluetooth Low Energy beacons in tandem with mobile devices and 3D panning on the web to overlay virtual sound arrangements onto a physical location in which users can listen through their phones and tablets. The virtual environments are distributed through a website and are populated with virtual sounds and speaker locations. Users activate the Bluetooth on their mobile devices to scan for beacons. User location navigates the corresponding location in virtual environment. Moving around the virtual environment probes the soundscape. The project also touches on issues of unreliability of Bluetooth tracking indoors, the state of BLE-based project development, and the need for Web Bluetooth development to enable these types of projects.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Mitchusson, Allison - 2018 - Lost In Space Indoor Localization for Virtual Environment Exploration.pdf:pdf},
type      = {Paper},
}

@InProceedings{2018_1,
author    = {Hespanhol, Nuno and Rodrigues, Óscar and Gomes, José Alberto},
title     = {0+1=SOM: Bringing Computing Closer to Children Through Music},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
pages     = {1--4},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {0 + 1 = SOM is a project developed by Digitópia - Casa da Música in cooperation with Braga Media Arts. In this project we developed a series of online tools that use technology and basic mathematical and logical concepts, such as counting and understanding a loop or an if condition, to create music. These tools were later used in a series of four workshops with elementary school children as a creative activity that complements the classroom. This paper describes the process and reasoning behind the creation of the tools (explaining our choices when creating the contents), the role Web Audio played in it (particularly in the ability to schedule precise audio events), the results we have achieved so far, and the feedback we have had from students and teachers. We also discuss further applications and plans for the future.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Hespanhol, Rodrigues, Gomes - 2018 - 01=SOM Bringing Computing Closer to Children Through Music.pdf:pdf},
keywords  = {composition,elementary education,intuitive manner,making music in an,mathematics,music,music education,software applications,that allow creating and,the increasing number of,this is related to},
type      = {Paper},
}

@InProceedings{2018_17,
author    = {Kleimola, Jari and Campbell, Owen},
title     = {Native Web Audio API Plugins},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
pages     = {4--9},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {This work enables native audio plugin development using the Web Audio API and other web technologies. Hybrid forms where DSP algorithms are implemented in both JavaScript and native C++, and distributed forms where web technologies are used only for the user interface, are also supported. Various implementation options are explored, and the most promising option is implemented and evaluated. We found that the solution is able to operate at 128 sample buffer sizes, and that the performance of the Web Audio API audio graph is not compromised. The proof-of-concept solution also maintains compatibility with existing Web Audio API implementations. The average MIDI latency was 24 ms, which is high when comparing with fully native plugin solutions. Backwards compatibility also reduces usability when working with multiple plugin instances. We conclude that the second iteration needs to break backwards compatibility in order to overcome the MIDI latency and multi-plugin support issues.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Kleimola, Campbell - 2018 - Native Web Audio API Plugins.pdf:pdf},
type      = {Paper},
url       = {https://www.youtube.com/watch?v=Kpv6_EU9YmE},
}

@InProceedings{2018_20,
author    = {Fiala, Jakub},
title     = {r-audio: Declarative, reactive and flexible Web Audio graphs in React Dependency on React},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {Web Audio is by design an object-oriented, imperative API offering low-level control over audio graphs. There have been a number of efforts to provide a more intuitive wrapper API. Designing such wrapper libraries poses challenges in addressing graph configuration, dynamic mutation and data flow. Syntax of creating directed graphs in imperative code is not representative of the complex graph shapes, making the code difficult to understand without external visualisation tools. In this paper I describe r-audio1, a Web Audio wrapper library which attempts to solve the issues of imperative graph representations by leveraging the component system of React. I compare approaches of existing wrapper libraries and discuss solutions to specific issues of declarative and reactive representations of Web Audio graphs. I evaluate r-audio in terms of the ability to create arbitrary directed graphs and mutate them in real time.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Fiala - 2018 - r-audio Declarative, reactive and flexible Web Audio graphs in React Dependency on React.pdf:pdf},
keywords  = {declarative,directed graphs,javascript,react,web audio},
type      = {Paper},
url       = {https://www.youtube.com/watch?v=cetQbIX3ji4},
}

@InProceedings{2018_4,
author    = {Thalmann, Florian and Thompson, Lucas and Sandler, Mark},
title     = {A User-Adaptive Automated DJ Web App with Object-Based Audio and Crowd-Sourced Decision Trees},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
volume    = {c},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {We describe the concepts behind a web-based minimal-UI DJ system that adapts to the user's preference via sim- ple interactive decisions and feedback on taste. Starting from a preset decision tree modeled on common DJ prac- tice, the system can gradually learn a more customised and user-specific tree. At the core of the system are structural representations of the musical content based on semantic au- dio technologies and inferred from features extracted from the audio directly in the browser. These representations are gradually combined into a representation of the mix which could then be saved and shared with other users. We show how different types of transitions can be modeled using sim- ple musical constraints. Potential applications of the system include crowd-sourced data collection, both on temporally aligned playlisting and musical preference.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Thalmann, Thompson, Sandler - 2018 - A User-Adaptive Automated DJ Web App with Object-Based Audio and Crowd-Sourced Decision Trees.pdf:pdf},
type      = {Paper},
}

@InProceedings{2018_22,
author    = {Buffa, Michel and Lebrun, Jerome and Kleimola, Jari and Larkin, Oliver and Pellerin, Guillaume and Letz, Stéphane},
title     = {WAP: Ideas for a Web Audio Plug-in Standard},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {Several native audio plug-in formats are popular today including Steinberg's VST, Apple's Audio Units, Avid's AAX and the Linux audio community's LV2. Although the APIs are different, all exist to achieve more or less the same thing - represent an instrument or audio effect and allow it to be loaded by a host application. In the Web Audio API such a high-level audio plug-in entity does not exist. With the emergence of web-based audio software such as digital audio workstations (DAWs), it is desirable to have a standard in order to make Web Audio instruments and effects interoperable. Since there are many ways of developing for Web Audio, such a standard should be flexible enough to support different approaches, including using a variety of programming languages. New functionality that is enabled by the web platform should be available to plug-ins written in different ways. To this end, several groups of developers came together to make their work compatible, and this paper presents the work achieved so far. This includes the development of a draft API specification, a small preliminary SDK, online plug-in validators and a set of examples written in JavaScript. These simple, proof of concept examples show how to discover plug-ins from repositories, how to instantiate a plug-in and how to connect plug-ins together. A more ambitious host has also been developed to validate the WAP standard: a virtual guitar “pedal board” that discovers plug-ins from multiple remote repositories, and allows the musician to chain pedals and control them via MIDI.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Buffa et al. - 2018 - WAP Ideas for a Web Audio Plug-in Standard.pdf:pdf},
type      = {Paper},
url       = {https://www.youtube.com/watch?v=5D5aQozfxvI},
}

@InProceedings{2018_7,
author    = {Sarwate, Avneesh and Tsuchiya, Takahiko and Freeman, Jason},
title     = {Collaborative Coding with Music: Two Case Studies with EarSketch},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {This paper describes the motivation, design, and implementation of new features in EarSketch that enable the collaborative creation of algorithmic music. EarSketch is a web-based Digital Audio Workstation (DAW), designed primarily for educational contexts, in which users author Python or JavaScript code to programmatically create music within a multi-track paradigm. In this paper, we describe these new collaborative features in EarSketch and discuss their potential for use in both educational and music performance contexts.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Sarwate, Tsuchiya, Freeman - 2018 - Collaborative Coding with Music Two Case Studies with EarSketch.pdf:pdf},
type      = {Paper},
url       = {https://www.youtube.com/watch?v=0qBVSCRpggg},
}

@InProceedings{2018_25,
author    = {Buffa, Michel and Lebrun, Jerome},
title     = {WebAudio Virtual Tube Guitar Amps and Pedal Board Design},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {In this paper are exposed our latest experiments with the WebAudio API to design different types of gears for guitarists: real-time simulations of tube guitars amplifiers, fx pedals, and their integration in a virtual pedal board. We have studied different real guitar tube amps and created an interactive Web application for experimenting, validating and building different amp designs that can be run in browsers. Blind tests have been conducted with professional guitar players who compared positively our real-time, low-latency, realistic tube guitar amps simulations with state-of-the-art native equivalents. We also created a set of “virtual audio fx pedals” that implement popular audio effects such as flanger, chorus, overdrive, pitch shifter etc. These amps and pedals simulations can be packaged as “WebAudio plugins” and stored in plugin repositories (REST endpoints or local folders). We also developed a “host” application -a virtual pedal board- that allows us to scan repositories for plugins and to chain/assemble them.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Buffa, Lebrun - 2018 - WebAudio Virtual Tube Guitar Amps and Pedal Board Design(3).pdf:pdf},
type      = {Paper},
}

@InProceedings{2018_10,
author    = {Major, Oliver},
title     = {DSP2JS A C ++ framework for the development of in-browser DSPs},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {We present DSP2JS, a new framework for the develop- ment of audio signal processors for web platforms using Em- scripten and the WebAudio API. In particular, the goal is to abstract common functionality in a configurable layer that manages the communication between a JavaScript applica- tion and DSP code written in C or C++. The framework includes functionality for the creation, connection and man- agement of processing units, runtime profiling, buffer man- agement, buffer conversion and a configurable build system. The proposed three-step development of a signal proces- sor with DSP2JS allows for external libraries to be included, making it possible to port existing code to the framework. The generated artifacts can then be used in a web page and invoked via an interface similar to native WebAudio- Nodes. The optional omission of WebAudio bindings via a bare-build mode potentially opens up the core framework to further DSP applications, even outside of the audio domain. We examine the multilayered architecture of the core framework and the build system, also discussing design and implemetation decisions.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Major - 2018 - DSP2JS A C framework for the development of in-browser DSPs.pdf:pdf},
type      = {Paper},
url       = {https://www.youtube.com/watch?v=n1a6sl-N9cQ},
}

@InProceedings{2018_13,
author    = {Larkin, Oliver and Harker, Alex and Kleimola, Jari},
title     = {iPlug 2: Desktop Plug-in Framework Meets Web Audio Modules},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
pages     = {1--6},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {This paper introduces iPlug 2: a desktop C++ audio plug-in framework that has been extended and reworked in order to support Web Audio Modules, a new format for browser-based audio effects and instruments, using WebAssembly. iPlug 2 provides a complete solution and workflow for the development of cross-platform audio plug-ins and apps. It allows the same concise C++ code to be used to create desktop and web-based versions of a software musical instrument or audio effect, including audio signal processing and user interface elements. This new version of the framework has been updated to increase its flexibility so that alternative drawing APIs, plug-in APIs and platform APIs can be supported easily. We have added support for the distributed models used in recent audio plug-in formats, as well as new graphics capabilities. The codebase has also been substantially modernised. In this paper, we introduce the problems that iPlug 2 aims to address and discuss trends in modern plug-in APIs and existing solutions. We then present iPlug 2 and the work required to refactor a desktop plug-in framework to support the web platform. Several approaches to implementing graphical user interfaces are discussed as well as creating remote editors using web technologies. A real-world example of a WAM compiled with iPlug 2 is hosted at https://virtualcz.io, a new web- based version of a commercially available synthesizer plug-in.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Larkin, Harker, Kleimola - 2018 - iPlug 2 Desktop Plug-in Framework Meets Web Audio Modules.pdf:pdf},
type      = {Paper},
url       = {https://www.youtube.com/watch?v=DDrgW4Qyz8Y},
}

@InProceedings{2018_16,
author        = {Favory, Xavier and Serra, Xavier},
title         = {Multi Web Audio Sequencer: Collaborative Music Making},
booktitle     = {Proceedings of the International Web Audio Conference},
year          = {2018},
editor        = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series        = {WAC '18},
address       = {Berlin},
month         = sep,
publisher     = {TU Berlin},
abstract      = {Recent advancements in web-based audio systems have enabled sufficiently accurate timing control and real-time sound processing capabilities. Numerous specialized music tools, as well as digital audio workstations, are now accessi- ble from browsers. Features such as the large accessibility of data and real-time communication between clients make the web attractive for collaborative data manipulation. However, this innovative field has yet to produce effective tools for multiple-user coordination on specialized music creation tasks. The Multi Web Audio Sequencer is a prototype of an application for segment-based sequencing of Freesound sound clips, with an emphasis on seamless remote collaboration. In this work we consider a fixed-grid step sequencer as a probe for understanding the necessary features of crowd-shared music creation sessions. This manuscript describes the sequencer and the functionalities and types of interactions required for effective and attractive collaboration of remote people during creative music creation activities.},
archiveprefix = {arXiv},
arxivid       = {arXiv:1905.06717v1},
eprint        = {arXiv:1905.06717v1},
file          = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Favory, Serra - 2018 - Multi Web Audio Sequencer Collaborative Music Making.pdf:pdf},
type          = {Paper},
}

@InProceedings{2018_3,
author    = {Carson, Tate},
title     = {A more perfect union: Composition with audience-controlled smartphone speaker array and evolutionary computer music},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {A more perfect union incorporates an audience-controlled smartphone speaker array with evolutionary computer mu- sic. A genetic algorithm drives the work and the perfor- mance practice that the audience follows.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Carson - 2018 - A more perfect union Composition with audience-controlled smartphone speaker array and evolutionary computer music.pdf:pdf},
type      = {Paper},
}

@InProceedings{2018_19,
author    = {Pauwels, Johan and Sandler, Mark B},
title     = {pywebaudioplayer: Bridging the gap between audio processing code in Python and attractive visualisations based on web technology},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2018},
editor    = {Monschke, Jan and Guttandin, Christoph and Schnell, Norbert and Jenkinson, Thomas and Schaedler, Jack},
series    = {WAC '18},
address   = {Berlin},
month     = sep,
publisher = {TU Berlin},
abstract  = {Lately, a number of audio players based on web technology have made it possible for researchers to present their audio-related work in an attractive manner. Tools such as wavesurfer.js, waveform-playlist and trackswitch.js provide highly-configurable players, allowing a more interactive exploration of scientific results that goes beyond simple linear playback. However, the audio output to be presented is in many cases not generated by the same web technologies. The process of preparing audio data for display therefore requires manual intervention, in order to bridge the resulting gap between programming languages. While this is acceptable for one-time events, such as the preparation of final results, it prevents the usage of such players during the iterative development cycle. Having access to rich audio players already during development would allow researchers to get more instantaneous feedback. The current workflow consists of repeatedly importing audio into a digital audio workstation in order to achieve similar capabilities, a repetitive and time-consuming process. In order to address these needs, we present pywebaudioplayer, a Python package that automates the generation of code snippets for the each of the three aforementioned web audio players. It is aimed at use-cases where audio develop- ment in Python is combined with web visualisation. Notable examples are Jupyter Notebook and WSGI-compatible web frameworks such as Flask or Django.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Pauwels, Sandler - 2018 - pywebaudioplayer Bridging the gap between audio processing code in Python and attractive visualisations based.pdf:pdf},
keywords  = {audio player,multi-track audio,python},
type      = {Paper},
}