
@InProceedings{2016_EA_39,
author    = {Houge, Ben},
title     = {Ornithological Blogpoem},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Ornithological Blogpoem is a setting of a poem by Elisa Gabbert for one or more voices and audience mobile devices, composed by Ben Houge. It receives its US premiere at the second Web Audio Conference on April 5, 2016, performed by the composer and a group of singers from Georgia Tech's student community, rehearsed by Jerry Ulrich and Timothy Hsu.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Houge - 2016 - Ornithological Blogpoem.pdf:pdf},
type      = {Performance},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54643/ornithological_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_69,
author    = {Sullivan, Joe},
title     = {Alternatives to Lookahead Audio Scheduling},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {The scheduling of web audio events occurs in the UI thread, which is optimized to respond to user input and to provide visual feedback. The setTimeout and setInterval interfaces provide an imprecise method of scheduling, and in background tabs the UI thread virtually ceases. Lookahead scheduling (à la A Tale of Two Clocks) is an established audio scheduling strategy, though it relies on the UI thread running continually. This talk surveys alternative scheduling strategies, including all-at-once scheduling and the pre-rendering of audio using the OfflineAudioContext (as described in A Tale of No Clocks), which tie the burden on the UI thread closely to user interactions. I discuss the general pattern pre-rendering implies through a demonstration of a proof-of-concept implementation, and explore the range of applications that suit pre-rendering, including the smallest of loop-based web tools (e.g. metronomes) and large-scale DAW projects where pre-rendering provides the added benefit of reducing computational demand during playback.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Sullivan - 2016 - Alternatives to Lookahead Audio Scheduling.pdf:pdf},
type      = {Talk},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54665/alternatives_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_44,
author    = {Gurtner, Corentin},
title     = {Applications of Audio and MIDI API Within a music notation editor},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {In this demo, I will show the web application Flat and the way we use Web Audio API and Web MIDI API for improving the collaborative score editing experience on flat.io. Flat is a collaborative score music editor. It allows people to create music from scratch by working in real-time with each other. It can be considered as a "Sibelius meets Google Doc meets Github" (although Flat does not allow to fork scores yet), and we designed it in order to be very easy to use. Each user has access to his personal space that lists all the scores he made. Each score can be private, public or open to some specific people. When a user invites other people to collaborate, he can see what these other users are doing in real-time, which parts they are working on, what notes they are setting ... etc. They can also communicate with each other through a real-time comments section. Once they composed an awesome piece, they can share it with their friends on social networks, but also with the Flat community and get likes, comments and requests to collaborate. We use an audio playback that is based on Chris Wilson's famous post "A tale of two clocks": However, all the notes in the next 2 seconds window are scheduled each 0.5 second in order to improve performance. Then a parser fetches all the necessary data to perfonn a realist audio playback, appropriate to the score. We save music scores in a custom JSON model lying on MusicXML format. Each instrument is made out of samples. We also plan to add some customizable synthesis-based instruments. Some envelopes specific to each instrument are set to allow smooth and clear transitions between notes. Some occasional effects (staccato, fermata, tremolo ... ) can also alter the envelope. The library responsible for audio processing is named Dacapo, and it's also used on the backend to export WAV, MP3 and MIDI files (through Node.js). We are also implementing MIDI devices support with a real-time display on the score during live recording. When you press a key of your controller, the note is displayed on the score immediately after the release, and you can then adjust the duration on a simple piano roll (very convenient to choose between a triplet and three quarter notes for example). Another feature we are going to release in the coming weeks is the ability to connect Flat to an output MIDI port. For instance, it can be very convenient to connect it to a virtual MIDI port. Then, users can enjoy a high-quality sound by connecting to a virtual instrument (like Kontakt) and enhance the unique Flat experience. The key feature of Flat, is the ability to collaborate with several people on the same music sheet (a bit like Google Docs) so I will show that feature with some of my friends in France. I will conclude by explaining what is coming next, like an audio recognition algorithm and physical modeling for instruments, though we need to wait for the Audio Worker to be released for such applications.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Gurtner - 2016 - Applications of Audio and MIDI API Within a music notation editor.pdf:pdf},
type      = {Demo},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54630/lightningtalks-day2_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_51,
author    = {Allik, Alo and Fazekas, György and Barthet, Mathieu and Sandler, Mark},
title     = {myMoodplay: An interactive mood-based music discovery app},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {myMoodplay is a web app that allows users to interactively discover music by selecting desired emotions. The application uses the Web Audio API, JavaScript animation for visualisation, linked data formats and affective computing technologies. We explore how artificial intelligence, semantic web and audio synthesis can be combined to provide new personalised online musical experiences. Users can choose degrees of energy and pleasantness to shape the desired musical mood trajectory. Semantic Web technologies have been embedded in the system to query mood coordinates from a triple store using a SPARQL endpoint and to connect to external linked data sources for metadata.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Allik et al. - 2016 - myMoodplay An interactive mood-based music discovery app.pdf:pdf},
type      = {Paper},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54589/myMoodplay_video.html?sequence=5&isAllowed=y},
}

@InProceedings{2016_EA_tut2,
author    = {Matuszewski, Benjamin and Schnell, Norbert and Goldszmidt, Samuel},
title     = {WavesJS Tutorial},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {This tutorial session will present WavesJS, a client-side low-level library dedicated to the audiovisual rendering of recorded audio signals and related data in the browser. The library has been designed with a strong focus on flexibility and modularity, and offers a set of extensible building blocks supporting a large range of applications. Along with the submitted paper, the workshop will present the general structure and architecture of the library (particularly the ui and audio components). A more in depth approach of the low level features will be achieved through the implementation of a simple application. WavesJS on GitHub: https://github.com/wavesjs},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Matuszewski, Schnell, Goldszmidt - 2016 - WavesJS Tutorial.pdf:pdf},
type      = {Tutorial},
}

@InProceedings{2016_EA_35,
author    = {Teaford, Luke},
title     = {Designing Synthesizers with Web Audio},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {43230},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Synthesis is an important part of Web Audio, and the native audio nodes give developers the power to build innovative and unique synthesizers. This talk will share knowledge and insight into synthesizer design and application architecture from the approach of making musical instruments gained from developing my Web Audio Synthesizers.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Teaford - 2016 - Designing Synthesizers with Web Audio.pdf:pdf},
type      = {Talk},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54659/designing_videostream.html?sequence=9&isAllowed=y},
}

@InProceedings{2016_EA_94,
author    = {Fields, Ben and Phippen, Sam},
title     = {rMIXr: how we learned to stop worrying and love the graph},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {In this talk we present a case study in the use of the web audio APIs. Specifically, our use of them for the creation of a rapidly developed prototype application. The app, called rMIXr (http://rmixr.com), is a simple digital audio workstation (DAW) for fan remix contests. We created rMIXr in 48 hours at the Midem Hack Day in June 2015. We'll give a brief demo of the app and show multi-channel sync. We'll also show various effects as well as cutting/time-slicing. Throughout the development process of this app we encountered difficulties in the web audio API and in some third party APIs. We overcame these difficulties through the use of some novel (dubious!) techniques. In particular we will speak to these issues: ∙ The existing web audio APIs have particular notions of how an audio graph should be built. We found violations of these notions in existing third party APIs. Where this abstraction boundary is violated, developers have to come up with their own notions of how to build a web audio graph. In this talk, we'll cover the abstractions we came up with for rMIXR, and speak to how they might generalize. We'll also cover some of the advantages and disadvantages that we discovered through these approaches. ∙ We'll speak to the construction of UIs that work for modification of the DAW and audio graph in real time. In particular, this will cover how we bind effect parameters into HTML controls. We'll also look at how we pass information from those controls down to the web audio graph and the layers between. ∙ As the app is for contests and fan use, being able to share state is a priority -- for this and other design reasons we wanted state preserved locally. How best to achieve this with state across many windows was non-obvious. We'll cover the approach that we took, which specifically takes advantage of the share-ability of URLs on the web. During the talk we will go through our solutions to these issues, showing by example a number of practical ways to get things done with the Web Audio APIs. We will walk through our abstractions and object models for building a fully functional DAW in the browser. These abstractions were necessary because we included a number of effects from third party libraries. Some of these effects libraries do not comply with the source and sink model that the web audio API predicates. Our abstractions gave us a more flexible graph structure that was critical to making this application work. While not perfect, it was certainly excellent for the rapid prototype scenario of the hackday. We'll also talk about some potential improvements to our abstractions. We will then show how to use localstorage to communicate across browser windows and how this approach can be harnessed to quickly create flexible UI elements with multiple windows. Lastly, and perhaps most ridiculously, we will discuss our use of the URL as a means to locally store all of the state of the web application. Here we will speak to why this is both the best and worst idea. All of the state for a particular rMIXr session is stored in the # part of the URL. This means that a current rMIXr session can be shared between users simply by copying the URL and sending it to someone else. We will aim to leave the audience with a collection of strategies they can use in their own web audio apps. We'll aim to provide a better understanding of some of the Web Audio API's structures, by way of example. The audience will also learn of an excellent means to discover what the maximum length of a URL can be before various modern browsers will crash (hint: it is quite large).},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Fields, Phippen - 2016 - rMIXr how we learned to stop worrying and love the graph.pdf:pdf},
type      = {Talk},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54668/rMIXr_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_62,
author    = {Mayer-Spohn, Ulrike and Takahashi, Keitaro},
title     = {Web application usingWeb Audio API for Recorderology - research concerning playing techniques of recorders},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {In this paper, we outline a documentation method used for our music research project, Recorderology, as developed to operate as a web application using Web Audio API. Our aim is to propose an application that enhances the knowledge and experiences of musicians, especially composers, about the recorder family and to encourage their creative activities. In our research project Recorderology, we analyze the correlations between the mechanisms and the actual results of sound production by means of four primary components (instrument model - air - mouth - fingers). This project is carried out through an interaction between artistic research involving collaborations with composers and musicians and scientific research including audio analysis, e-learning and data-mining. In our Web application, we employ a large audio database to describe the mechanism of the playing techniques of recorders along with a graphic user interface with the aim of simplifying the navigation.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Mayer-Spohn, Takahashi - 2016 - Web application usingWeb Audio API for Recorderology - research concerning playing techniques of recorde.pdf:pdf},
keywords  = {Instrumentation,Recorder Instrument,Web Audio,e-learning},
type      = {Paper},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54593/lightningtalks-day2_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_68,
author    = {Lee, Sang Won and Essl, Georg},
title     = {Hooking up Web Audio to WebGL Typography},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {48109},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {This demo introduces programmable text rendering that enables temporal typography in web browsers. Textual interaction is seen not only as a dynamic but interactive process facilitating both scripted and live musical expression in various contexts such as audio-visual performance using keyboards and live coding visualization. We transform plain text into a highly audiovisual medium and a musical interface which is visually expressive by transforming textual properties using real-time web audio signal. Technical realization of the concept uses Web Audio API, WebGL and GLSL shaders. We show a number of examples that illustrate instances of the concept in various scenarios ranging from simple textual visualization, live coding environments and interactive writing platform.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Lee, Essl - 2016 - Hooking up Web Audio to WebGL Typography.pdf:pdf},
type      = {Demo},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54633/lightningtalks-day2_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_18,
author    = {Su, David},
title     = {Geneva},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Geneva is an interactive exploration of genetic algorithms as applied to sonification of tweets, which are scraped in real time and converted to music using sentiment analysis. The work is in many ways a musical adaptation of and homage to Karl Sims' Genetic Images (1993); to facilitate the listener/user's simultaneous evaluation of multiple melodies, each chromosome is placed in a 3D space, allowing for different combinations to be heard depending on the player's location. In addition, the first-person controls allow for easy control and manipulation of both sonic (mute, solo) and genetic (select, reject, evolve) aspects of the population. Mutation and crossover algorithms, which affect pitch, rhythm, and timbre as well as the tweet content itself, are heavily influenced by John Biles' GenJam. In addition to Web Audio API (timbre.js), Geneva makes use of WebGL (THREE.js) and the Twitter API.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Su - 2016 - Geneva.pdf:pdf},
isbn      = {978-0-692-61973-5},
type      = {Artwork},
}

@InProceedings{2016_19,
author    = {Vieilleribière, Adrien},
title     = {Improving time travel experience by combining annotations},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Since recorded audio material is played, navigating rele- vantly through it is a key expectation. This paper provides a formalism to introduce flexible navigation systems based on sets of annotations applying to the same audio object. It aims to build web interfaces to explore audio in time, robust for large data-sets and long files. Introducing the concept of weights applied to annotations, it specifies a parameterized version of the functionality next/previous and presents an effective implementation.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Vieilleribi{\`{e}}re - 2016 - Improving time travel experience by combining annotations.pdf:pdf},
type      = {Paper},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54578/lightningtalks-day2_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_71,
author    = {Thalmann, Florian and Perez Carillo, Alfonso and Fazekas, György and Wiggins, Geraint A. and Sandler, Mark B.},
title     = {The Semantic Music Player: A Smart Mobile Player Based on Ontological Structures and Analytical Feature Metadata},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {1--6},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {The Semantic Music Player is a cross-platform web and mobile app built with Ionic and the Web Audio API that explores new ways of playing back music on mobile devices, particularly indeterministic, context-dependent, and interactive ways. It is based on Dynamic Music Objects, a format that represents musical content and structure in an abstract way and makes it modifiable within definable constraints. For each Dynamic Music Object, the Semantic Music Player generates a custom graphical interface and enables appropriate user interface controls and mobile sensors based on its requirements. When the object is played back, the player takes spontaneous decisions based on the given structural information and the analytical data and reacts to sensor and user interface inputs. In this paper, we introduce the player and its underlying concepts and give some examples of the potentially infinite amount of use cases and musical results.},
doi       = {ISBN: 978-0-692-61973-5},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Thalmann et al. - 2016 - The Semantic Music Player A Smart Mobile Player Based on Ontological Structures and Analytical Feature Metadata.pdf:pdf},
type      = {Paper},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54596/semantic_video.html?sequence=5&isAllowed=y},
}

@InProceedings{2016_91,
author    = {Allison, Jesse and Holmes, Daniel and Berkowitz, Zachary and Pfalz, Andrew and Conlin, William and Hwang, Nick},
title     = {Programming Music Camp: Using Web Audio to Teach Creative Coding},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Programming Music Camp is a summer outreach camp designed to teach computer programming concepts to youths through the activity of music-making. Prior experiences teaching web audio technologies to secondary school students are described. The camp curriculum is then outlined, including the class activities of live coding, instrument design, and concert performance. The outcomes of the camp are evaluated and future educational opportunities using web audio technologies are considered.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Allison et al. - 2016 - Programming Music Camp Using Web Audio to Teach Creative Coding.pdf:pdf},
type      = {Paper},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54601/lightningtalks-day2_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_15,
author    = {Bundin, Andrey},
title     = {Concert for Smartphones},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {The performance involves audience participation with their mobile devices. Connected to a wireless network and organized into one polyphonic multichannel synthesizer, those devices reproduce different noises, samples, and synthesized sounds from random locations in the hall. In addition, loudspeakers complement the phone choir to add dynamic climaxes and low-frequency effects. The name of the performance refers to academic music traditions. The author emphasizes the similarity in musical nature of the Concert for Smartphones and any classical concert for soloist and orchestra. The performance is an experiment in exploring possible ways of using an audience's mobile devices as a medium for sound diffusion in the context of a traditionally composed electroacoustic musical piece.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Bundin - 2016 - Concert for Smartphones.pdf:pdf},
isbn      = {978-0-692-61973-5},
keywords  = {Abstract,Mobile devices,Performance,Polyphonic synthesizer,Proceedings,Video,Web audio},
type      = {Performance},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54639/concert_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_8,
author    = {Mckegg, Matt},
title     = {Building Desktop Apps using Web Audio},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Building a full featured desktop audio application is a difficult and time consuming task. There are many decisions to make, such as the amount of dependency on existing frameworks, and a cross-platform strategy. The level of detail required to produce a good user experience normally involves a large team of developers and designers. I have been using computers to make music for the last 10 years. More recently, I started making the transition from bedroom composition to live performance. Becoming increasingly frustrated with existing tools for live electronic music performance, I started to embark on a journey to create my own application. In my day job, I am a web application developer and am familiar with JavaScript, Node, HTML and CSS. I did not want to learn a new toolset, so was very excited to discover the Web Audio API. I could use familiar tools to build a user interface, a familiar language to build the logic of my application, and the Web Audio API to actually do the audio DSP work. However, while the web provides an excellent developer experience and rapid prototyping, it does not provide a solid foundation for a live music performance. Software for live music needs to be offline and have full file system access. Electron is a runtime created by GitHub that allows developers to package up JavaScript and run it as a standalone desktop application. It combines Chromium (the open source project behind Google Chrome) and Node.js (a tool for writing JavaScript that runs on servers) and provides the complete desktop application development experience I was searching for. In my talk, I will demonstrate my application Loop Drop, and show how the combination of Web and Desktop technologies creates an ideal single developer/small team environment for building new audio applications. I will also discuss some of the challenges in doing so.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Mckegg - 2016 - Building Desktop Apps using Web Audio.pdf:pdf},
type      = {Talk},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54656/building-desktop_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_37,
author    = {Costello, Edward and Lazzarini, Victor and Timoney, Joseph},
title     = {Constructing AudioUnit Plugins on the Web using Csound},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {This paper describes a web-based application which can be used to construct AudioUnit plugins. Using this application the audio DSP component of an AudioUnit plugin can be created using the Csound audio programming language, and the user interface (UI) composed using HTML5. This is made possible using the combination of a Csound binary compiled for Google's portable native client API (PNaCl) which runs Csound code inside of the web application, and the Csound AudioUnit API which allows Csound compiled code to run inside of native AudioUnit plugins. The application allows users to live-code an AudioUnit plugin using on-the-fly Csound code and HTML5 evaluation within the browser which allows rapid testing of the plugin audio DSP and UI components. A common Javascript interface allows the plugin UI to communicate with both the web-based PNaCl Csound and the native AudioUnit encapsulated Csound instances. This common interface uses a JSON representation of the plugin control parameters and presets allowing users to control the plugin and create presets that function inside of both the web application and the AudioUnit.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Costello, Lazzarini, Timoney - 2016 - Constructing AudioUnit Plugins on the Web using Csound.pdf:pdf},
type      = {Paper},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54582/constructing_videostream.html?sequence=10&isAllowed=y},
}

@InProceedings{2016_81,
author    = {Lambert, Jean-Philippe and Robaszkiewicz, Sébastien and Schnell, Norbert},
title     = {Synchronisation for Distributed Audio Rendering over Heterogeneous Devices, in HTML5},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {6--11},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {The HTML5 standard is wide-spread on mobile devices. In combination with the Web Audio API, it allows for massively distributed real-time audio rendering. But timing issues exist, mainly because of the lack of standard inter-device synchronisation. This paper proposes a synchronisation solution based on HTML5. Using a shared reference time, we achieved the distributed rendering of audio events with an individual accuracy of 1 to 10 ms, 5 ms in standard deviation, which is more accurate than the audio block duration, for any device that we measured},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Lambert, Robaszkiewicz, Schnell - 2016 - Synchronisation for Distributed Audio Rendering over Heterogeneous Devices, in HTML5.pdf:pdf},
keywords  = {Audio,Distributed,HTML5,Synchronisation,Web Audio API},
type      = {Paper},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54598/synchronisation_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_45,
author    = {Collins, Tom and Coulon, Christian},
title     = {Using Empirical Analysis of Music Corpora to Optimize Web Audio Playback},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {1--4},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Due to feasibility issues and musical preferences, Web audio applications have tended to emphasize the use of synthesized instruments and short samples (e.g., drums) over large banks of longer files that sample other acoustic instruments such as a violin or piano. As the sounds generated by sampled acoustic instruments are quite realistic, they are likely to be of interest to many users of Web audio applications. Using the Tone.js Web Audio framework, this paper describes an initial investigation into load times when rendering music with such sampled instruments. A method is proposed for reducing load times, and hence optimizing Web Audio playback, based on empirical analysis of the note durations used across different music corpora. Experimental results for 400 randomly selected short music excerpts indicate that the proposed method does lead to significant load time reductions, from 3.87 s to 1.72 s. Researchers interested in replicating the results of these experiments or downloading and exploring our playback solution are pointed to http://tomcollinsresearch.net/research/wac/2016/},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Collins, Coulon - 2016 - Using Empirical Analysis of Music Corpora to Optimize Web Audio Playback.pdf:pdf},
type      = {Paper},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54585/lightningtalks-day2_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_11,
author    = {Wallace, Tony},
title     = {WebX0X - Web Audio Drum Synthesizer and Sequencer},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {WebX0X is a drum synthesizer and sequencer built using the Web Audio API. All sound generation is performed entirely in the browser without the use of samples. WebX0X takes its name from the classic drum machines made by Roland in the 1980s, most notably the TR-606, TR-707, TR-808 and TR-909. Like the 606 and 808, WebX0X synthesizes all its sounds using simple oscillators, noise generators, filters and envelope generators and has an integrated step sequencer to control patterns. WebX0X has 4 synthesizers, or voices'. Unlike most classic analog drum machines, WebX0X uses the same algorithm for all voices. Each voice is tuned to resemble a different kind of drum by default (kick, snare, hi-hat and cowbell), but is also capable of a wide variety of other percussive sounds. WebX0X makes use of a combination of standard Web Audio API features such as OscillatorNode, BiquadFilterNode and GainNode, and custom components including an AHDSR envelope generator and variable-rate noise generator. WebX0X has been featured by several prominent online publications, including Create Digital Music and FactMag. The latest iteration, available at https://webx0x.com, allows users to save and optionally make their patterns (beats) publicly visible and shareable.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Wallace - 2016 - WebX0X - Web Audio Drum Synthesizer and Sequencer.pdf:pdf},
type      = {Demo},
}

@InProceedings{2016_EA_77,
author    = {Dale, John Henry},
title     = {Streaming BinauralFIR Node Audio With 360 Video},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {23036},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {In moving towards the development of a technical standard for streaming 360 degree audio-visual media and using the Ricoh Theta SDK as a development platform, the author will present a demonstration of a browser-based iOS application using a live, streaming monaural audio signal paired with 360 degree video, with both signals being generated by the Ricoh Theta S camera. The audio signal is intended to be spatialized in 3 dimensions using the BinauralFIR node developed for the Web Audio API by Arnau Juliá Collados in his degree's thesis Design of a binaural synthesis processor in the browser using Web Audio API (with supervision by T. Carpentier, S. Goldszmidt and F. Vallverdu). A browser-based application of this type appears to be one of the intended use cases of the BinauralFIR node: The BinauralFIR node can be useful to create e.g, an audio streaming application with binaural listening controlled by the user as it is showed in the figure 3.6. In this example, an audio streaming such as e.g., a live radio concert emitted in multi-track is get from a server. (Collados, 2014: 28) The main goal of the demonstration will be to use the BinuralFIR node to stream live audio from the Ricoh Theta S microphone in real- time, as opposed to streaming a previously recorded audio file as shown in the Binural FIR Example demonstration presented at WAC 2015 by T. Carpentier here: http://ircam-rnd.github.io/binauralFIR/examples/ Although Google/Youtube appears to be working towards a real-time 360 video stitching standard (http://bzfd.it/1VOX5Wh) , the Ricoh Theta S, due to inherent hardware limitations, is not currently capable of this task. The live 360 video stream for this demonstration will therefore be presented as two 180 degree fisheye videos arranged side-by-side in a 1920 x 1080 video player window.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Dale - 2016 - Streaming BinauralFIR Node Audio With 360 Video.pdf:pdf},
isbn      = {3054286762},
type      = {Demo},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54636/lightningtalks-day1_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_KN2,
author    = {Melchior, Frank},
title     = {Brave New World -- Experiences in Next Generation Audio Broadcasting},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {The Internet has had a huge impact on the way we live our lives and the level of impact on the broadcast industry is no different. Our audiences now fully expect access to content on demand. They may browse the web while watching programmes. Smartphones and tablets are more and more often the go-to devices for all online activities. In the last decade broadcasters have made countless changes to accommodate this new environment. The introduction and success of BBC iPlayer is just one example. As the Internet continues to influence all aspects of society, its importance to the future of the BBC is paramount. BBC R&D's vision for the future of broadcasting, content is produced and broadcast over Internet Protocol (IP). Using IP to create and deliver content, we envisage a world where we can offer media experiences that are very different to those of today. These experiences will enable increasingly immersive experiences that are more: ∙ Personal -- know and understand the requirements of individuals and change the experience accordingly. ∙ Adaptive -- recognise the device being used and adapt to give the best experience in real time, regardless of the manufacturer. ∙ Dynamic and responsive -- respond to the needs of the audience in terms of length, depth of interest, location, preferences, lifestyle and age. ∙ Interactive - the audience can select specific areas of content to focus on and in some instances create and upload their own associated content. BBC R&D has already started to deliver this vision and the standards, capabilities and tools to enable it. Much of this work builds on the characteristics of IP that allow us to treat programmes as a set of objects of content. These new representations can adapt to individual audience members' requirements using the browser as a key media consumption instrument. We refer to this work as object-based broadcasting. This talk will focus on the audio dimension. It will illustrate the vision of object-based audio in broadcasting and highlight the importance of client side audio processing to enable new audience experiences.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Melchior - 2016 - Brave New World – Experiences in Next Generation Audio Broadcasting.pdf:pdf},
isbn      = {978-0-692-61973-5},
keywords  = {Abstract,Proceedings,Video,WAC 2016 Keynote,Web audio},
type      = {Keynote},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54674/BraveNewWorld_videostream.html?sequence=9&isAllowed=y},
}

@InProceedings{2016_EA_59,
author    = {Kachalo, Sema},
title     = {JZZ.js - a unified API for MIDI applications},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {There are multiple ways to access and emulate MIDI devices in JavaScript. This includes real MIDI access like Web-MIDI API or Jazz-Plugin, Web-Audio synths like MIDI.js or Timbre.js, or HTML gadgets for MIDI input like Qwerty-Hancock. When used all together, these methods allow good OS/browser coverage; however, it is cumbersome to use them all in the same project because of their API differences. We introduce JZZ.js - a JavaScript library that provides unified interface for all possible MIDI implementations and has the following key features: - Hiding the details of asynchronous calls behind the developer-friendly chaining syntax. e.g. JZZ().openMidiOut(/Yamaha/).send(0x90,60,127).wait(500).send(0x80,60,127); - Treating MIDI inputs and outputs as graph nodes that can be directly connected (inspired by the Web-Audio API design). e.g. JZZ().openMidiIn().connect(JZZ().openMidiOut('Microsoft GS Wavetable Synth')); - Numerous helpers for converting MIDI messages to/from the human-readable form. e.g. console.log(JZZ.MIDI.aftertouch(0,'D#5',127).toString()); - Extensibility. We provide examples how to adapt the developers' own libraries and gadgets into the framework. JZZ.js works with Node.js and all major browsers in Windows, MacOS, and Linux. Limited support is available for Android and iOS. The library is free to download at https://github.com/jazz-soft/JZZ},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Kachalo - 2016 - JZZ . js - a unified API for MIDI applications.pdf:pdf},
type      = {Talk},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54664/jzzJS_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_38,
author    = {Laguna, Christopher and Lerch, Alexander},
title     = {Client-Side Audio Declipping},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Clipping is an unpleasant recording artifact that occurs when an audio signal's level rises above a microphone's or AD converter's maximum input level. As more audio and video recordings are being taken on mobile devices (sometimes in high sound level conditions such as live concerts), clipping has become an issue that users encounter frequently. We present ClipAway: a web application that analyzes an audio file and automatically removes clipping from the audio file. The following scenario illustrates the service we supply: 1) A user attends a live concert and creates an audio recording of the concert. 2) The user listens to the recording at home and notices clipping, which causes the listening experience to be unsatisfactory. 3) The user uploads the audio recording to the ClipAway website. 4) Audio processing occurs in the browser, and the user then exports a quality-enhanced version of the recording. 5) The listening experience with the resulting audio file has significantly improved. The advantage of browser-based processing is that it is more familiar and accessible to most users than a native solution, which would likely require the user to install a standalone software or a digital audio workstation hosting a plugin for quality enhancement. The declipping algorithm is split into two sections: clipping detection and clipping correction. Clipping detection involves the automatic estimation of the clipping level and the subsequent localization of clipped regions. The clipping level is determined by identifying anomalies in the signal's amplitude histogram near the positive and negative endpoints. The locations of clipping are determined by identifying samples with amplitudes close to the clipping level where the signal has a near-horizontal slope. Clipping correction involves replacing short clipped regions using spline interpolation and replacing long clipped regions through linear interpolation of time-frequency bin magnitudes.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Laguna, Lerch - 2016 - Client-Side Audio Declipping.pdf:pdf},
type      = {Demo},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54629/lightningtalks-day1_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_49,
author    = {Dias, Bruno and Pinto, H Sofia and Matos, David M},
title     = {BPMTimeline : JavaScript Tempo Functions and Time Mappings using an Analytical Solution},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
number    = {1},
series    = {WAC '16},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Time mapping is a common feature in many (commercial and/or open-source) Digital Audio Workstations, allowing the musician to automate tempo changes of a musical performance or work, as well as to visualize the relation between score time (beats) and real/performance time (seconds). Unfortunately, available music production, performance and remixing tools implemented with web technologies like JavaScript and Web Audio API do not offer any mechanism for flexible, and seamless, tempo manipulation and automation. In this paper, we present BPMTimeline, a time mapping library, providing a seamless mapping between score and performance time. To achieve this, we model tempo changes as tempo functions (a well documented subject in literature) and realize the mappings through integral and inverse of integral of tempo functions.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Dias, Pinto, Matos - 2016 - BPMTimeline JavaScript Tempo Functions and Time Mappings using an Analytical Solution.pdf:pdf},
keywords  = {automation,javascript,tempo function,time mapping},
type      = {Paper},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54588/BPMTimeline_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_66,
author    = {Lee, Sang Won and de Carvalho, Antonio Deusany Jr. and Essl, Georg},
title     = {Crowd in C[loud]},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {48109},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Crowd in C[loud] is an audience participation music piece played on a distributed musical instrument. Inspired by Terry Riley's In C, audience members play a short tunes composed by themselves on their smartphones. The collective outcome of the ensemble creates a heterophonic texture of chance, largely in C chord. The instrument mimics an online dating website in which a user browses personal profiles, likes someone, and mingle with other online users. Participants are guided to play music together and to interact with other audience members in this temporary social network. A performer can actively progress the music by orchestrating the crowd by live coding on the console of the web browser. To participate, visit http://bit.ly/crowdinc with your smartphone on Firefox, Chrome, or Safari.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Lee, de Carvalho, Essl - 2016 - Crowd in Cloud.pdf:pdf},
isbn      = {978-0-692-61973-5},
keywords  = {Abstract,Musical profiles,Networked interactive music,Performance,Proceedings,Video,Web audio},
type      = {Performance},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54644/crowd_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_89,
author    = {Dunn, Brian},
title     = {Reactive Audio},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2577},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {React has revolutionized the way client side developers think about their applications. It makes heavy use of the principles of functional reactive programming. How can these design concepts aid in the development of live music applications? With React, events effect state, and the library implements side effects that keep the presentation synchronized. This allows for a simplicity and determinism that restores joy to the process of building complex interactions. We will look at how this powerful idea can be applied to real time music synthesis, rendering music as a side effect of state change. Timing is everything when rendering music. We will look at some different approaches to achieving precise timing in a reactive programming model. These approaches include push-based, where changes to state precipitate re-rendering, and pull-based, where state is constantly rendered and changes to state are reflected on the next render. The pros and cons of these approaches will be examined. In terms of the WebAudio API, all of these changes find their way to automating the values of an AudioParam sooner or later. We will discover some potential improvements to this class that could simplify and clarify implementations in this problem domain. This will be a practical approach. There will be examples in ClojureScript and (hopefully) Elm. Live code execution and glorious rhythmic beeping will keep things fascinating.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Dunn - 2016 - Reactive Audio.pdf:pdf},
type      = {Talk},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54667/reactiveaudio_video.html?sequence=5&isAllowed=y},
}

@InProceedings{2016_61,
author    = {Bernstein, Andrew and Taylor, Benjamin},
title     = {Gendy.js: A Web Audio Module for Dynamic Stochastic Synthesis},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {This paper outlines Gendy.js, a JavaScript Web Audio implementation of the GENDYN program for dynamic stochas- tic synthesis originally authored by composer Iannis Xenakis. The historical development of dynamic stochastic synthesis is reviewed followed by an overview of it's technical components and their implementation in JavaScript. Finally, the authors examine the possibilities and issues regarding future development of nonstandard synthesis algorithms using the Web Audio API.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Bernstein, Taylor - 2016 - Gendy.js A Web Audio Module for Dynamic Stochastic Synthesis.pdf:pdf},
type      = {Paper},
}

@InProceedings{2016_EA_54,
author    = {Paradis, Matthew and Pike, Chris and Day, Richard and Melchior, Frank},
title     = {A Novel Approach to Streaming and Client Side Rendering of Multichannel Audio with Synchronised Metadata},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
volume    = {2076},
series    = {WAC '16},
pages     = {2076},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Object based audio broadcasting is an approach which combines audio with metadata that describes how the audio should be rendered. This metadata can include spatial positioning mixing parameters and descriptors to define the type of audio represented by the object. In this user-led interactive demo we show an approach to enabling the streaming of multichannel audio and synchronised metadata to the browser. Audio is rendered in the browser to multiple formats based on the information contained in the synchronised metadata channel. This allows adaptive mixing and rendering of content and user interaction. Based on the MPEG/DASH standard this approach allows an arbitrary number of audio channels to be presented as discrete inputs to the Web Audio API (dependent on any channel limit imposed by the browser). Binaural, 5.1 and stereo renders can be generated and selected for output by the user in real time without any change to the source media stream. Channels marked as being interactive can have their properties exposed to the user to adjust based on their preferences. The audio and metadata is originated from a single BWF file compliant with ITU-R BS 2076 (Audio Definition Model) with the audio being encoded using AAC (as per the MPEG/DASH standard) and the metadata represented in JSON format to the browser. This approach provides a flexible framework for the prototyping and presentation of new audio experiences to online audiences and provides a platform for delivery object based audio to online users.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Paradis et al. - 2016 - A Novel Approach to Streaming and Client Side Rendering of Multichannel Audio with Synchronised Metadata.pdf:pdf},
isbn      = {978-0-692-61973-5},
keywords  = {Abstract,Audio metadata,Object based audio broadcasting,Proceedings,Video,Web audio},
type      = {Talk},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54661/ANovelApproach_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_30,
author    = {Goldszmidt, Samuel and Vincent, Renaud},
title     = {Personal-JS},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Personal-JS is a web application based on the Web Audio API and Web RTC standards where connected users play synchronously loops of a well-known song -- Personal Jesus by Depeche Mode -- together. The user creates or joins a jam room, and then can play some audio loops with other users. The application is based on Sync library (http://github.com/collective-soundworks/sync) to synchronize the different devices through the PeerJS WebRTC wrapper (http://peerjs.com) and thus requires no web server, except for serving the audio loops. (As iOS doesn't support Web RTC standard yet in their mobile browser, iOS users can't play with this application. We also encourage people to listen to the lyrics while they will face their Black Mirror).},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Goldszmidt, Vincent - 2016 - Personal-JS.pdf:pdf},
type      = {Artwork},
}

@InProceedings{2016_EA_82,
author    = {Lee, Sang Won and Martinez, Mari and Essl, Georg and Pain},
title     = {Live Writing: Shatter},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {48109},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Live Writing: Shatter utilizes an audiovisual performance developed for a web browser. Here, every keystroke made on a laptop to write a piece of creative writing is captured and precessed to established natural links among typing gestures, text written, and audiovisual responses. The piece is built upon a poem written by Martinez about fragments of memories left by unspoken words. The performance capitalizes on temporal typography built on a web browser using Web Audio API, and Web Graphics Library, offering unique text animation sculpted with live audio to the audience.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Lee et al. - 2016 - Live Writing Shatter.pdf:pdf},
type      = {Performance},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54654/livewriting_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_tut4,
author    = {Frisson, Christian and Pietrzak, Thomas and Zhao, Siyan and Schwemler, Zachary and Israr, Ali},
title     = {WebAudioHaptics: Tutorial on Haptics with Web Audio},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {The Web Audio Haptics WAC Tutorial 2016 will explore how to create meaningful haptic content that engages different areas of the body using off-the-shelf hardware and open source software running on a web browser using Web Audio technologies. Participants will 1) learn the basic theories of tactile illusions; 2) get an overview on actuators and sensors; 3) explore tactile illusions using web-based audio tools and a box connecting actuators and sensors to their computer audio I/O; and 4) ideate use cases in groups. Tutorial material will remain available from: http://github.com/WebAudioHaptics},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Frisson et al. - 2016 - WebAudioHaptics Tutorial on Haptics with Web Audio.pdf:pdf},
type      = {Tutorial},
}

@InProceedings{2016_EA_72,
author    = {Sigal, Jason},
title     = {OLOS: Visual Music Programming},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {10003},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Olos is a graphical, web-based platform to play with music through code. It is a collection of modular web components: a visual programming environment, and a set of stand-alone components. Olos components provide a GUI layer of abstraction atop existing Web Audio API functionality. Within the Olos environment, the components become draggable and connectable. Global constraints such as tempo and key encourage musicality. Immediate auditory and visual feedback facilitates improvisation and computational thinking. Each component's source code can be modified directly with JavaScript in a live coding overlay. Emerging web technologies make the building blocks of audio accessible through the browser. Olos makes these elements more approachable for users who may not have experience with code or music.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Sigal - 2016 - OLOS Visual Music Programming.pdf:pdf},
type      = {Demo},
}

@InProceedings{2016_67,
author    = {Jillings, Nicholas and De Man, Brecht and Moffat, David and Reiss, Joshua D and Stables, Ryan},
title     = {Web Audio Evaluation Tool: A framework for subjective assessment of audio},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
number    = {July 2015},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Perceptual listening tests are commonplace in audio research and a vital form of evaluation. While a large number of tools exist to run such tests, many feature just one test type, are platform dependent, run on proprietary software, or require considerable configuration and programming. Using Web Audio, the Web Audio Evaluation Tool (WAET) addresses these concerns by having one toolbox which can be con-figured to run many different tests, perform it through a web browser and without needing proprietary software or computer programming knowledge. In this paper the role of the Web Audio API in giving WAET key functionalities are shown. The paper also highlights less common features, available to web based tools, such as easy remote testing environment and in-browser analytics.},
doi       = {10.1525/aa.1955.57.2.02a00090},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Jillings et al. - 2016 - Web Audio Evaluation Tool A framework for subjective assessment of audio.pdf:pdf},
isbn      = {00027294},
issn      = {15481433},
type      = {Paper},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54595/lightningtalks-day2_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_84,
author    = {Winters, R Michael and Tsuchiya, Takahiko and Lerner, Lee W and Freeman, Jason},
title     = {Multi-Modal Web-Based Dashboards for Geo-Located Real-Time Monitoring},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {This paper describes ongoing research in the presentation of geo-located, real-time data using web-based audio and visualization technologies. Due to both the increase of devices and diversity of information being accumulated in real-time, there is a need for cohesive techniques to render this information in a useable and functional way for a variety of audiences. We situate web-sonification---sonification of web-based information using web-based technologies---as a particularly valuable avenue for display. When combined with visualizations, it can increase engagement and allow users to profit from the additional affordances of human hearing. This theme is developed in the description of two multi-modal dashboards designed for data in the context of the Internet of Things (IoT) and Smart Cities. In both cases, Web Audio provided the back-end for sonification, but a new API called DataToMusic (DTM) was used to make common sonification operations easier to implement. DTM provides a valuable framework for web-sonification and we highlight its use in the two dashboards. Following our description of the implementations, the dashboards are compared and evaluated, contributing to general conclusions on the use of web-audio for sonification, and suggestions for future dashboards.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Winters et al. - 2016 - Multi-Modal Web-Based Dashboards for Geo-Located Real-Time Monitoring.pdf:pdf},
type      = {Paper},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54600/multi-modal_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_7,
author    = {Mckegg, Matt},
title     = {Live Looping Electronic Music Performance with MIDI Hardware},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {I have spent much of the last three years building a Web Audio based desktop application for live electronic music performance called Loop Drop. It was inspired by my own frustration with existing tools for live performance. I wanted a tool that would give me, as a performer, the level of control and expression desired but still feel like playing a musical instrument instead of programming a computer. My application was built using web technologies such as JavaScript and HTML5, leveraging existing experience as a web developer and providing an excellent workflow for quick prototyping and user interface design. In combination with Electron, a single developer can build a desktop audio application very efficiently. Loop Drop uses Web MIDI to interface with hardware such as Novation Launchpad. The software allows creation of sounds using synthesis and sampling, and arranges these into chunks which may be placed in any configuration across the midi controller's button grid. These sounds may be triggered directly, or played quantised to the current tempo at a given rate using beat repeat. Everything the performer plays is collected in a buffer that at any time may be turned into a loop. This allows the performer to avoid recording anxiety --- a common problem with most live looping systems. They can jam out ideas, then once happy with the sequence, press the loop button to lock it in. In my performance, I will use Loop Drop in conjunction with multiple Novation Launchpad midi controllers, to improvise 15 minutes of electronic music using sounds that I have organised ahead of time. The user interface will be visible to the audience as a projection. I will also be demonstrating the power of hosting Web Audio in Electron by interfacing with an external LED array connected over Serial Peripheral Interface Bus (SPI) to be used as an audio visualiser light show.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Mckegg - 2016 - Live Looping Electronic Music Performance with MIDI Hardware.pdf:pdf},
type      = {Performance},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54638/livelooping_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_12,
author    = {Bundin, Andrey},
title     = {Virtual Sound Gallery},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Virtual Sound Gallery (VSG) is a web stage for modern multichannel music, sound, and audiovisual art. It is an accessible, web-based virtual reality (VR) environment for a visualized binaural simulation of multichannel sound reproduction. In this environment, a user can change their location among virtual loudspeakers and rotate their head to get the best spatial listening experience. In addition, an integrated video engine provides the ability to play visual content on one or several virtual screens in sync with the audio. VSG provides access to different electroacoustic music compositions presented in several virtual exhibitions and classified by concepts, styles, and organizations. Technically, VSG is a one-page website developed with modern Java Script, PHP, and MySQL. VSG works on a modern desktop and mobile browsers. It is also compatible with such virtual reality devices as Oculus Rift and Google Cardboard. Further development of the system includes adding algorithmic composition functionality, a sound objects spatialization approach, panoramic video engine, static addresses of works and exhibitions, comments, and personal messages.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Bundin - 2016 - Virtual Sound Gallery.pdf:pdf},
type      = {Artwork},
}

@InProceedings{2016_33,
author    = {Mahadevan, Anand and Freeman, Jason and Magerko, Brian},
title     = {An interactive, graphical coding environment for EarSketch online using Blockly and Web Audio API},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {This paper presents an interactive graphical programming environment for EarSketch, using Blockly and Web Audio API. This visual programming element sidesteps syntactical challenges common to learning text-based languages, thereby targeting a wider range of users in both informal and academic settings. The implementation allows seamless integration with the existing EarSketch web environment, saving block-based code to the cloud as well as exporting it to Python and JavaScript.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Mahadevan, Freeman, Magerko - 2016 - An interactive, graphical coding environment for EarSketch online using Blockly and Web Audio API.pdf:pdf},
type      = {Paper},
}

@InProceedings{2016_EA_79,
author    = {Madhavan, Nihar and Snyder, Jeff},
title     = {Constellation: A Musical Exploration of Phone-Based Audience Interaction Roles},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {1--4},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {With Constellation, we explore a system for audience interaction with performers during a music performance. We seek to reconfigure the relationship between the audience and performers by designing a system for interaction during a music performance using mobile devices. Additionally, we aim to augment the listening experience in a music performance by using mobile devices as speakers. Constellation was designed and built for the Spring 2015 Princeton Laptop Orchestra concert. We designed four sections to the piece, exploring various roles played by performers and the audience. We conclude by discussing the results and suggesting potential future work.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Madhavan, Snyder - 2016 - Constellation A Musical Exploration of Phone-Based Audience Interaction Roles(2).pdf:pdf},
type      = {Performance},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54645/constellation_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_23,
author    = {Freeman, Jason},
title     = {Live Coding With EarSketch},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {1504293},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {EarSketch combines a Python / JavaScript API, a digital audio workstation (DAW) visualization, an audio loop library, and an educational curriculum into a web-based music programming environment. While it was designed originally as a classroom educational tool for music technology and computer science, it has recently been expanded to support live coding in concert performance. This live coding performance explores the artistic potential of algorithmic manipulations of audio loops in a multi-track DAW paradigm and explores the potential of DAW-driven visualizations to demystify live coding and algorithms for a concert audience.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Freeman - 2016 - Live Coding With EarSketch.pdf:pdf},
type      = {Performance},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54640/livecoding_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_42,
author    = {Fradkin, Scott},
title     = {A Block Based Live Coding Music Environment for Kids},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
volume    = {16},
number    = {figure 2},
series    = {WAC '16},
pages     = {53590},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {There are a number of live coding environments for creating music. The vast majority of these require the user to be able to read and write and to be able to understand complex syntax. Scratch is a very popular environment for kids to learn how to code. It uses blocks rather than just words to indicate commands. This reduces the amount of reading and typing a child has to do and adds a visual element to creating code. What if we combine the ease of use of a block based programming environment with live coding of music? Using the combination of Snap (a JavaScript clone of Scratch) plus Tone.js, I'm building a live coding environment for music that kids can use. I will discuss my motivation for building this tool, discuss and demo a couple of iterations of the tool, talk about the various issues I had to work through with Snap and Tone.js, and also what I'd like to add to the tool to make it better. Browse to the project page at http://www.fradkin.com/snap-music.html to see the current version of the environment.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Fradkin - 2016 - A Block Based Live Coding Music Environment for Kids.pdf:pdf},
type      = {Talk},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54660/block-based_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_41,
author    = {Ziya, Ehsan},
title     = {Complex Rhythmic Structures with Beet.js},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {6--8},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {This paper describes Beet.js, a new Javascript library for creating complex and layered rhythmical structures in the browser using the Web Audio API. Link to source: github.com/zya/beet.js Link to demo page: zya.github.io/beet.js},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Ziya - 2016 - Complex Rhythmic Structures with Beet.js.pdf:pdf},
keywords  = {euclidean rhythm,javascript,polyrhythm,web audio api},
type      = {Paper},
}

@InProceedings{2016_EA_KN1,
author    = {Thorington, Helen},
title     = {Start, Stop, Begin Again},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {This talk is in two parts. The first part is about my journey toward a career in sound and radio production -- a journey of many years full of related and unrelated starts, stops, and fresh beginnings. The second part is about the founding of New American Radio, a series of half hour radio programs by American artists that aired on the National Public Radio network from 1989-1998, and about the founding of Turbulence.org, where over the past 20 years, over 300 new media works have been commissioned. I plan to discuss a few of the works from Turbulence's earlier years and show the movement away from familiar art forms -- literary and musical -- and on toward a networked practice. The vulnerability of creative work in the digital medium is noted. In recognition of the longer life span of works in print, I had planned to call this talk: Why I Am Going to Write a Book.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Thorington - 2016 - Start, Stop, Begin Again.pdf:pdf},
type      = {Keynote},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54675/StartStopBegin_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_90,
author    = {Vinay, Ashvala and Boulanger, Richard},
title     = {Building Interactive Systems with WebSockets and Csound},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {In this demo, we show interactive systems that use Csound's web-audio related builds -- Csound for Portable Native Client (PNaCl), and Emscripten as their primary sound synthesis engine; and a server side build based on Node.js. We use WebSockets and modern implementations of Socket-based transports such as Socket.io to connect participants seamlessly to each other. Our demos explore three use cases -- a collaborative sound design tool; a networked music performance system over the internet; and a networked music performance system over a local area network. We will also demonstrate the differences between using Csound as the synthesis engine on the server side vs using Csound as the synthesis engine on the client side. The end result is a robust interactive networked music system that can be used from multiple platforms.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Vinay, Boulanger - 2016 - Building Interactive Systems with WebSockets and Csound.pdf:pdf},
type      = {Demo},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54637/lightningtalks-day2_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_92,
author    = {Allison, Jesse and Ostrenko, Derick and Cellucci, Vincent A},
title     = {Causeway},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {5622},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Causeway is an interactive poetry app and performance written by Vincent A. Cellucci with audio by Jesse Allison and visuals by Derick Ostrenko. Originally a part of Cellucci's book, An Easy Place / To Die (CityLit 2011), the poem "Causeway" was inspired by events following Hurricane Katrina. The piece can be experienced as a performance or by itself as a mobile application/installation. When Causeway---a 2-screen experience--- is put on as a performance, Cellucci performs a reading of the poem while audience members interact by touching phrases from the poem on their mobile devices to collectively transform visuals displayed on a large projection. Each tap produces a sonic echo taken from Cellucci's voice and causes his words to ripple through the theater. As an application, this experience is containerized on the mobile device so that many users over time contribute to a collective visualization. Software utilized includes: JavaScript, HTML, CSS, Node.js, OSC, Tone.js, Socket.IO, OpenStack, iOS, Max, and Android.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Allison, Ostrenko, Cellucci - 2016 - Causeway.pdf:pdf},
type      = {Artwork},
}

@InProceedings{2016_EA_36,
author    = {Walker, William and Belet, Brian},
title     = {Cross-Town Traffic 2.0},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Cross-Town Traffic 2.0 is an ensemble music performance environment for any number of audience performers, a principal performer, and a conductor. The performers use their own mobile devices running a performance interface based on the Web Audio API. The conductor leads the performers through a fully composed musical structure. Sixteen previously recorded audio files (eight Hammond B3 samples, performed and recorded by Walker; and eight viola samples, performed and recorded by Belet) are arranged into four groups, with the audience performers similarly arranged in four corresponding performance sections. Following cues from the conductor, the ensuing performance immerses humans in the midst of cellphone speakers and the flow of the musical structure. Individual performers can shape their own audio contribution within the confines of the larger composed structure, providing an element of playful participation. The resulting distributed cellphone audio challenges the performance roles of the humans in the room, as opposed to the number and quality of loudspeakers in the space. Using mobile web audio offers very low barriers to audience participation, in contrast to logging into an app store, searching and finding the appropriate native app, installing and launching the app, all prior to the start of the performance.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Walker, Belet - 2016 - Cross-Town Traffic 2.0.pdf:pdf},
type      = {Performance},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54642/cross-town_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_58,
author    = {Holzborn, Damon},
title     = {Capsule, A Modular Step Sequencer for Web Audio and MIDI},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Capsule is a modular composition and performance system developed with the Web Audio and Web MIDI APIs. It is an attempt to combine the conceptual simplicity of a modular-style step sequencer with the algorithmic flexibility of a live coding language. Capsule presents a starting point that is familiar to a wide range of electronic musicians, and aims to be friendly for those new to electronic music as well. On top of this foundation is a collection of modules that can be added to a capsule to modify the progression of the sequence, add algorithmic modification to the sequence, and provide interaction methods for use in live performance. Each module is individually simple to learn and use; collectively, the modules build on each other flexibly to provide depth to the system. Capsule is designed to allow users to start immediately with an accessible, musically open sequencer. Over time, users can develop sophisticated compositional and performative strategies, with each stage designed to facilitate a rewarding music-making experience.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Holzborn - 2016 - Capsule, A Modular Step Sequencer for Web Audio and MIDI.pdf:pdf},
type      = {Talk},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54663/capsule_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_28,
author    = {Bundin, Andrey},
title     = {Virtual Sound Gallery: a web stage for modern multichannel music and multimedia art},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Virtual Sound Gallery (VSG) is a web stage for modern multichannel music, sound, and audiovisual art. It is an accessible, web-based virtual reality (VR) environment for a visualized binaural simulation of multichannel sound reproduction. In this environment, a user can change their location among virtual loudspeakers and rotate their head to get the best spatial listening experience. In addition, an integrated video engine provides the ability to play visual content on one or several virtual screens in sync with the audio. VSG provides access to different electroacoustic music compositions presented in several virtual exhibitions and classified by concepts, styles, and organizations.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Bundin - 2016 - Virtual Sound Gallery a web stage for modern multichannel music and multimedia art.pdf:pdf},
type      = {Demo},
}

@InProceedings{2016_48,
author    = {Dias, Bruno and Davies, Matthew E P and Matos, David M and Pinto, H Sofia},
title     = {Time Stretching & Pitch Shifting with the Web Audio API: Where are we at?},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Audio time stretching and pitch shifting are operations that all major commercial and/or open source Digital Audio Workstations, DJ Mixing Software and Live Coding Suites offer. These operations allow users to change the duration of audio files while maintaining the pitch and vice-versa. Such operations enable DJs to speed up or slow down songs in order to mix them by aligning the beats. Unfortunately, there are few (and experimental) client-side JavaScript implementations of these two operations. In this paper, we review the current state of the art for client-side implementations of time stretching and pitch shifting, their limitations, and describe new implementations for two well-known algorithms: (1) Phase Vocoder with Identity Phase Lock and (2) a modified version of Overlap & Add. Additionally, we discuss some issues related to the Web Audio API (WAA) and frequency-based audio processing regarding latency and audio quality in pitch shifting and time stretching towards raising awareness about possible changes in the WAA.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Dias et al. - 2016 - Time Stretching & Pitch Shifting with the Web Audio API Where are we at.pdf:pdf},
type      = {Paper},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54587/timestretching_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_80,
author    = {Guttandin, Christoph},
title     = {Non Audio Signal Processing with the Web Audio API},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {I recently ported a Shazam-like application to create and compare fingerprints of audio files from Python to JavaScript. Although this application handles audio data or data derived from audio data, only a very small part of it actually uses the Web Audio API. In fact I used the Web Audio API only to derive the PCM data of the input file. My aim was to rewrite the code in a way that it uses the Web Audio API for all the necessary steps to finally compute the fingerprints. Those steps include tasks like filtering, limiting or normalizing the output of the FFT transformations. In Python most of this is done by using the NumPy package or the SciPy library. Both of them are dedicated libraries for scientific and numerical computation. With the Web Audio API those tasks could be executed by using the functionality of the DynamicsCompressorNode, the GainNode, the WaveShaperNode and the IIRFilterNode. All other cases could be handled by an AudioWorklet or a ScriptProcessorNode respectively. I started by using the OfflineAudioContext to process non audio data and discovered that it can be roughly thought of as the browser's stream implementation which we don't have yet. Besides the advantage of reusing already implemented functionality, it is also potentially faster then any custom implementation and has the benefit of offloading the computation to another thread without adding any additional complexity by setting up a custom WebWorker. With the new suspend() and resume() methods of the OfflineAudioContext, it is possible to render data as it arrives, although the final size of the rendered AudioBuffer needs to be known before creating the OfflineAudioContext. I want to summarize my findings and show what's possible in current browser's implementation. A simple example for that could be that almost any numerical data can be processed. Surprisingly an AudioBuffer can happily handle values from -4294967296 to 4294967296, which will be suitable for many types of signals. I would like to conclude my talk by briefly showing other non audio uses of the Web Audio API. One of those examples could be a package called webaudio-serial-tx published by substack. It sends serial data via the audio output. Another example for using the Web Audio API in a creative way is a project by Daniel Rapp called doppler which is detecting motion by playing frequencies above the audible range and analyzing their response.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Guttandin - 2016 - Non Audio Signal Processing with the Web Audio API.pdf:pdf},
type      = {Talk},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54666/nonaudiosignal_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_57,
author    = {Martinez, Juan Carlos and Freeman, Jason},
title     = {A JavaScript Pitch Shifting Library for EarSketch with Asm.js},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {0--2},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {A JavaScript pitch shifting library based on asm.js was developed for the EarSketch website. EarSketch is a Web Audio API-based educational website that teaches computer science principles through music technology and composition. Students write code in Python and JavaScript to manipulate and transform audio loops in a multi-track digital audio workstation paradigm. The pitch-shifting library provides a cross-platform, client-side pitch-shifting service to EarSketch to change the pitch of audio loop files without modifying their playback speed. It replaces a previous server-side pitch-shifting service with a noticeable increase in performance. This paper describes the implementation and performance of the library transpiled from a set of basic DSP routines written in C and converted to Asm JavaScript using emscripten.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Martinez, Freeman - 2016 - A JavaScript Pitch Shifting Library for EarSketch with Asm.js.pdf:pdf},
type      = {Paper},
}

@InProceedings{2016_EA_46,
author    = {Buffa, Michel and Demetrio, Maxime and Azria, Nouriel},
title     = {Guitar pedal board using WebAudio},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {21428511},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {The proposed demo is a guitar pedal board coded from scratch and inspired by the Guitar FX Chrome Application1. The demo is functional and proposes a graph based GUI for assembling effects such as: guitar amplifier emulation, Auto Wah, Delay, Distortion/Overdrive, Reverb, Chorus, etc. It includes a preset management system and saves incrementally, client side, any change in the parameters of the current preset. The latency is system and browser-dependent, but on a Mac Book Pro / Google Chrome the latency is as low as 14ms, making it playable in real time. The MVVC design is interesting for modularity and makes easy adding new effects. During the demo a guitar will be plugged into a computer using an external sound card, and the pedal board will process the sound in real-time. Lots of efforts have been put on the GUI and ease of use of the application, as well as on the core effect: the guitar amplifier simulation. The project is called Guitar Processor and is open source2.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Buffa, Demetrio, Azria - 2016 - Guitar pedal board using WebAudio.pdf:pdf},
type      = {Demo},
}

@InProceedings{2016_EA_34,
author    = {Taylor, Ben and Shanahan, Daniel and Wolf, Matthew and Allison, Jesse T. and Baker, David John},
title     = {reNotate: The Crowdsourcing and Gamification of Symbolic Music Encoding},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Musicologists and music theorists have, for quite some time, hoped to be able to make use of computational methods to examine large corpora of music. As far back as the 1940s, an IBM card-sorter was used to implement patternfinding in traditional British folk songs (Bronson 1949, 1959). Alan Lomax famously implemented statistical methods in his Cantometrics project (Lomax, 1968), which sought to collate a large corpus of folk music from across many cultures. In the 1980s and 90s, a number of encoding projects were instituted in an attempt to be able to make searchable music notation on a large scale. The Essen Folksong Collection (Schaffrath, 1995) collected ethnographic transcriptions, whereas projects at the Center for Computer Assisted Research in the Humanities (CCARH) focused on scores in the Western Art Music tradition (Bach chorales, Mozart sonatas, instrumental themes, etc.). Recently, scholars have focused on improving Optical Music Recognition, in the hopes of facilitating the acquisition of large numbers of musical scores (Fujinaga, et al., 2014), but non-notated music, such as improvisational jazz, is often overlooked. While there have been many advances in music information retrieval in recent years, parameters that would facilitate in-depth musicological analysis are still out of reach (for example, stream segregation to examine specific melodic lines, or the analysis of harmony at a resolution that would allow for an analysis of specific chord voicings). Our project seeks to implement methods similar to those used in CAPTCHA and RECAPTCHA technology to crowdsource the symbolic encoding of musical information through a web-based gaming interface. The introductory levels ask participants to tap along with an audio recording's tempo, giving us an approximate BPM, while the second level asks for participants to tap with onsets. The third level asks them to match a contour of a three-note segment, and the final stage asks for specific note matching within that contour. A social-gaming interface allows for users to compete against one another. It is our hope that this work can be generalized to many types of musical genres, and that a web-based framework might facilitate the encoding of musicological and music-theoretic datasets that might be underrepresented by current MIR work.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Taylor et al. - 2016 - reNotate The Crowdsourcing and Gamification of Symbolic Music Encoding.pdf:pdf},
type      = {Talk},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54658/renotate_video.html?sequence=5&isAllowed=y},
}

@InProceedings{2016_EA_tut5,
author    = {Wallace, Tony},
title     = {Introduction to Percussion Synthesis Using Web Audio},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Introduction to Percussion Synthesis Using Web Audio will introduce participants to the basics of web audio programming. This tutorial will begin with a discussion of the audio graph. Participants will construct a simple graph by connecting standard nodes, including the OscillatorNode, BiquadFilterNode and GainNode, and learn how to generate white noise. The second part of the tutorial will introduce the AudioParam object. Participants will learn how to schedule changes to AudioParam values by creating an attack-decay (AD) envelope generator. The third part of the tutorial will demonstrate how the previous exercises can be combined into a flexible percussion synthesizer. Participants should come prepared with a computer with a plain text and web browser (Chrome, Firefox or Safari) installed and should have basic familiarity with JavaScript. About the presenter: Tony Wallace is a software developer specializing in web and mobile applications, and a former professional musician, music teacher and instructional video author. He is the president of Irritant Creative Inc. and creator of the WebX0X drum synthesizer and sequencer (https://webx0x.com). Tony resides in Oakville, Ontario Canada.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Wallace - 2016 - Introduction to Percussion Synthesis Using Web Audio.pdf:pdf},
type      = {Tutorial},
}

@InProceedings{2016_EA_70,
author    = {Sullivan, Joe},
title     = {Schroeder: A Web Audio Musical Instrument},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Schroeder is a hardware musical instrument (a standalone Wurlitzer clone) built on the Web Audio API. Applications for the Web Audio API will rarely utilize the API alone, but must instead integrate with other technologies/APIs. This is true for websites using web audio as well as hardware projects. This project explores the particular challenges of using web audio to build a simple hardware musical instrument, from fitting a computer in a box with speakers and a MIDI keyboard to implementing a web interface with no screen.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Sullivan - 2016 - Schroeder A Web Audio Musical Instrument.pdf:pdf},
type      = {Demo},
}

@InProceedings{2016_65,
author    = {Lee, Sang Won and Deusany de Carvalho Junior, Antonio and Essl, Georg},
title     = {Crowd in C[loud]: Audience Participation Music with Online Dating Metaphor using Cloud Service},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {1--6},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {In this paper, we introduce Crowd in C[loud], a networked music piece designed for audience participation at a music concert. We developed a networked musical instrument for the web browser where a casual smartphone user can play music as well as interact with other audience members. A participant composes a short tune with five notes and serving as a personal profile picture of each individual throughout the piece. The notion of musical profiles is used to form a social network that mimics an online-dating website. People browse the profiles of others, choose someone they like, and initiate interaction online and offline. We utilize a cloud service that helps build, without a server-side programming, a large-scale networked music ensemble on the web. This paper introduces the design choices for this distributed musical instrument. It describes details on how the crowd is orchestrated through the cloud service. We discuss how it facilitates mingling with one another. Finally we show how live coding is incorporated while maintaining the coherence of the piece. From rehearsal to actual performance, the crowd takes part in the process of producing the piece.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Lee, Deusany de Carvalho Junior, Essl - 2016 - Crowd in Cloud Audience Participation Music with Online Dating Metaphor using Cloud Servi.pdf:pdf},
type      = {Paper},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54594/audience_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_87,
author    = {Roberts, Charles},
title     = {Improvisation in Gibber},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
volume    = {9},
number    = {2},
series    = {WAC '16},
pages     = {593--599},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {This performance employs recent research on pattern manipulation and representation in live-coding performance practice. Using Gibber, a browser-based live-coding environment, I create rhythmic and melodic patterns and sequence their subsequent transformations. Gibber dynamically updates the source code of performances to reflect these transformations, and also injects additional code annotations depicting the current phase of musical sequences and their most recent output. The end result of these annotations is a dynamic source code document, constantly shifting and changing to reveal algorithmic processes to audiences.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Roberts - 2016 - Improvisation in Gibber.pdf:pdf},
type      = {Performance},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54655/improvisation_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_55,
author    = {Tsuchiya, Takahiko and Freeman, Jason and Lerner, Lee W.},
title     = {Data-Driven Live Coding with DataToMusic API},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Creating interactive audio applications for web browsers often involves challenges such as time synchronization between non-audio and audio events within thread constraints and format-dependent mapping of data to synthesis parameters. In this paper, we describe a unique approach for these issues with a data-driven symbolic music application programming interface (API) for rapid and interactive development. We introduce DataToMusic (DTM) API, a data-sonification tool set for web browsers that utilizes the Web Audio API1 as the primary means of audio rendering. The paper demonstrates the possibility of processing and sequencing audio events at the audio-sample level by combining various features of the Web Audio API, without relying on the ScriptProcessorNode, which is currently under a redesign. We implemented an audio event system in the clock and synthesizer classes in the DTM API, in addition to a modular audio effect structure and a exible data-to-parameter mapping interface. For complex real-time configuration and sequencing, we also present a model system for creating reusable functions with a data-agnostic interface and symbolic musical transformations. Using these tools, we aim to create a seamless connection between high-level (musical structure) and low-level (sample rate) processing in the context of real-time data sonification.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Tsuchiya, Freeman, Lerner - 2016 - Data-Driven Live Coding with DataToMusic API.pdf:pdf},
keywords  = {data sonification,live coding,modulation,real-time clock,web audio api},
type      = {Paper},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54590/data-driven_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_27,
author    = {Taylor, Benjamin and Bernstein, Andrew},
title     = {Tune.js: A Microtonal Web Audio Library},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {3--6},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {The authors share Tune.js, a JavaScript library of over 3,000 microtonal tunings and historical temperaments for use with web audio. The current state of tuning in web audio is reviewed, followed by an explication of the library's creation and an overview of its potential applications. Finally, the authors share several small projects made with Tune.js and ponder future development opportunities.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Taylor, Bernstein - 2016 - Tune . js A Microtonal Web Audio Library.pdf:pdf},
type      = {Paper},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54580/lightningtalks-day2_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_tut1,
author    = {Adenot, Paul},
title     = {Workshop: Optimizing and Debugging Web Audio API Applications},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {In this tutorial, we will look at two different aspects of working with the Web Audio API. First, we'll have a look into the performance characteristics of the different AudioNodes available, their performance profile, overall CPU and memory cost, and strategies to use resources (CPU, memory) more efficiently. For example, we'll learn how to look into the source code of different implementations, and determine the algorithm and techniques used in each browser, to make better choices when developing applications. We'll then look into ways to make processing lighter, while still retaining the essence of the application, for example to make a "degraded" mode for mobile. We'll use techniques such as substituting rendering methods to trade fidelity against CPU load, pre-baking assets, minimizing resampling. Throughout the workshop, we'll use tools and techniques to debug audio problems, both using in-browser tools, or JavaScript code designed to inspect static and dynamic audio graphs and related Web Audio API objects.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Adenot - 2016 - Workshop Optimizing and Debugging Web Audio API Applications.pdf:pdf},
type      = {Tutorial},
}

@InProceedings{2016_93,
author    = {Roma, Gerard and Simpson, Andrew J R and Grais, Emad M and Plumbley, Mark D},
title     = {Remixing musical audio on the web using source separation},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {4},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Research in audio source separation has progressed a long way, producing systems that are able to approximate the component signals of sound mixtures. In recent years, many efforts have focused on learning time-frequency masks that can be used to filter a monophonic signal in the frequency domain. Using current web audio technologies, time-frequency masking can be implemented in a web browser in real time. This allows applying source separation techniques to arbitrary audio streams, such as internet radios, depending on cross-domain security configurations. While producing good quality separated audio from monophonic music mixtures is still challenging, current methods can be applied to remixing scenarios, where part of the signal is emphasized or de-emphasized. This paper describes a system for remixing musical audio on the web by applying time-frequency masks estimated using deep neural networks. Our example prototype , implemented in client-side Javascript, provides reasonable quality results for small modifications.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Roma et al. - 2016 - Remixing musical audio on the web using source separation.pdf:pdf},
type      = {Paper},
}

@InProceedings{2016_EA_26,
author    = {Weitnauer, Michael and Meier, Michael},
title     = {bogJS -- A JavaScript framework for object-based rendering in browsers},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {687645},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {With the introduction of HTML5 and the Web Audio API, an important prerequisite was made for native rendering of object-based audio in modern browsers. Object-based audio is a revolutionary approach for creating and deploying interactive, personalized, scalable and immersive content, by representing it as a set of individual assets together with metadata describing their relationships and associations. This allows media objects to be assembled in ground-breaking ways to create new user experiences. This talk will introduce the open-source framework bogJS, developed by IRT (Institut für Rundfunktechnik) within the scope of the EU funded project ORPHEUS, which utilizes native nodes of the Web Audio API to realize such experiences. One aim of the development was to provide a flexible API that can be easily extended with different types of user interfaces and various representations of object metadata. Another key aim of the framework is to offer different possibilities to access audio signals for the rendering by taking into account current drawbacks and limitations of browser such as the supported number of tracks in a media element, the ordering of decoded multi-channel tracks or synchronized playback issues. Interoperability and implementation of current and upcoming standards, such as the ITU-R BS.2076 recommendation for a metadata definition model, will be taken into account and are currently under development. Furthermore, first projects and demonstrations using the framework will be presented.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Weitnauer, Meier - 2016 - bogJS – A JavaScript framework for object-based rendering in browsers.pdf:pdf},
type      = {Talk},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54657/bogJS_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_40,
author    = {Walker, William and Belet, Brian},
title     = {Musique Concrète Choir: An Interactive Performance Environment for Any Number of People},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {1--5},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Using the Web Audio API, a roomful of smartphones becomes a platform on which to create novel musical experiences. As seen at WAC 2015, composers and performers are using this platform to create clouds of sound distributed in space through dozens of loudspeakers. This new platform offers an opportunity to reinvent the roles of audience, composer, and performer. It also presents new technology challenges; at WAC 2015 some servers crashed under load. We also saw difficulties creating and joining private WiFi networks. In this piece, building on the lessons of WAC 2015, we load all our sound resources onto each phone at the beginning of the piece from a stable, well-known web host. Where possible, we use the new Service Worker API to cache our resources locally on the phone. We also replace real-time streaming control of roomful of phones with real-time engagement of the audience members as performers.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Walker, Belet - 2016 - Musique Concr{\`{e}}te Choir An Interactive Performance Environment for Any Number of People.pdf:pdf},
type      = {Paper},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54583/musique_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_86,
author    = {Mann, Yotam and Rothberg, Sarah},
title     = {Jazz.Computer},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {11237},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {Jazz.Computer is an interactive song in which the listener's scrolling affects the timbre, arrangement and progress of the music. We chose scrolling as the interaction both because it is often the first thing users do when arriving at a web page and because Jazz.Computer recontextualizes a Facebook-style news feed to in order to transform passive information consumption into active musical engagement. The song itself is about these kinds of information feeds and their effects those of us who are always scrolling: sometimes uplifting, sometimes disheartening. The audio is all triggered and synthesized live using Tone.js, a Web Audio framework, allowing every part of the song to vary in response to the user's input. Musical parameters like tempo, envelopes, waveforms, and effects are interpolated depending on the user's vertical scroll position on the page. As the song speeds and slows down, the arrangement of the song changes to match the tempo; at the top of the page, the song is four-on-the-floor dance beat and at the bottom, Jazz.Computer has a laid-back triplet feel with a gradient of textures in between. Jazz.Computer's visuals are all created by THREE.js, a WebGL library. Each instrument has a corresponding geometric element which is triggered along with the sound. Jazz.Computer is an open-ended composition which is different for each player and on each listen. It demonstrates a participatory musical experience enabled by the Web Audio API and distributed through the browser.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Mann, Rothberg - 2016 - Jazz.Computer.pdf:pdf},
isbn      = {978-0-692-61973-5},
type      = {Artwork},
}

@InProceedings{2016_83,
author    = {Matuszewski, Benjamin and Schnell, Norbert and Goldszmidt, Samuel},
title     = {Interactive Audiovisual Rendering of Recorded Audio and Related Data with the WavesJS Building Blocks},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {This article presents a set of components for the interactive audiovisual rendering of recorded audio signals and related data streams (e.g. audio descriptors and annotations) together with a set of example applications. The components are based on SVG graphics and the Web Audio API. The construction of both, the graphical user interface and the audio rendering of an application relies on a small hierarchical structure of classes that formalize different aspects of the rendering and facilitate both the implementation of complex applications using the provided components and the extension of the library by further defined graphics and audio rendering components. The library and the example applications described in the article are freely available.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Matuszewski, Schnell, Goldszmidt - 2016 - Interactive Audiovisual Rendering of Recorded Audio and Related Data with the WavesJS Building.pdf:pdf},
keywords  = {audio pro-,audio visualization,cessing,graphical user interface,html 5,web audio api},
type      = {Paper},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54599/interactive_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_32,
author    = {Taylor, Benjamin},
title     = {The Last Cloud},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {The Last Cloud is a live net art performance which reflects on the web browser as a source of form and content. The performance uses live coding to control several browser windows with HTML media, including HTML5 audio and video players, Google Maps, GIFs, and images. This media is mashed up and glitched in a narrative media collage. The work is accompanied by a web audio composition which uses samples, synthesis, text-to-speech, and processed media element streams to put the sound of web browsing in a musical context. The Last Cloud is inspired by the history of media artists who have turned media for reproducing content into instruments for generating content. In this case, the web browser, which was first used as an avenue for viewing reproductions of 20th century media (such as newspapers, photographs, and recorded songs), is used as an instrument for glitching, splicing, and collaging those media, creating a new form of art which could not exist in any other medium.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Taylor - 2016 - The Last Cloud.pdf:pdf},
type      = {Performance},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54641/lastcloud_video.html?sequence=5&isAllowed=y},
}

@InProceedings{2016_EA_56,
author    = {Su, David},
title     = {meSing.js: A JavaScript Singing Synthesis Library},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {meSing.js is a JavaScript singing synthesis library that uses the Web Audio API's DSP capabilities in conjunction with the meSpeak.js speech synthesis library to provide a vocal synthesizer for the web. First, the lyrics with corresponding MIDI notes are parsed and fed to meSpeak.js; the resulting text-to-speech output is then converted into a series of AudioBufferSourceNodes, which are subsequently processed and adjusted for pitch, rhythm, and expression. Pitchshifting techniques currently implemented are: feeding the synthesized audio through a multiband vocoder (based on Chris Wilson's 2012 demo), directly adjusting the audio playback rate, and manipulating the "pitch" parameter of the meSpeak.js synthesizer. Rhythmic adjustments occur directly on the PCM level via slicing and concatenating the Float32Arrays containing audio channel data, as well as using the Web Audio API's clock to schedule vocoder events. The demo showcases an example usage of meSing.js: a songwriting tool that provides both lyrical and melodic suggestions, using the singing synthesis to rapidly prototype the vocal line. The step sequencer-like input grid layout is derived from musicologist Kyle Adams' approach to interpreting and analyzing hip hop, and is particularly well suited to lyric and rhyme analysis. Multiple approaches were taken and experimented with while developing meSing.js, each with its own performance and usability benefits and drawbacks; a discussion of these approaches can provide insight into creating libraries atop the Web Audio API.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Su - 2016 - meSing . js A JavaScript Singing Synthesis Library.pdf:pdf},
type      = {Talk},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54662/lightningtalks-day2_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_tut3,
author    = {Schnell, Norbert and Lambert, Jean-philippe and Goldszmidt, Samuel and Matuszewski, Benjamin},
title     = {Soundworks Tutorial},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
pages     = {2016},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {The Soundworks framework is dedicated to the development of applications featuring co-located collaborative/collective mobile interactions. The framework is entirely based on web APIs and Node.js. It provides a set of services, abstractions, and user interfaces that help the developer to concentrate on the essential -- the design of interaction and audiovisual rendering. In the tutorial, we will present the features and architecture of the framework as well as some examples before guiding the attendees through the implementation of a simple application. The workshop will conclude with a brief collective performance. Soundworks on GitHub: https://github.com/collective-soundworks/soundworks},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Schnell et al. - 2016 - Soundworks Tutorial.pdf:pdf},
type      = {Tutorial},
}

@InProceedings{2016_47,
author    = {Perez-Carrillo, Alfonso and Thalmann, Florian and Fazekas, György and Sandler, Mark},
title     = {Geolocation Adaptive Music Player},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
series    = {WAC '16},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {We present a web-based cross-platform adaptive music player that combines music information retrieval (MIR) and audio processing technologies with the interaction capabilities offered by GPS-equipped mobile devices. The application plays back a list of music tracks, which are linked to geographic paths in a map. The music player has two main enhanced features that adjust to the location of the user, namely, adaptable length of the songs and automatic transitions between tracks. Music tracks are represented as data packages containing audio and metadata (descriptive and behavioral) that builds on the concept of Digital Music Object (DMO). This representation, in line with next-generation web technologies, allows for flexible production and consumption of novel musical experiences. A content provider assembles a data pack with music, descriptive analysis and action parameters that users can experience and control within the restrictions and templates defined by the provider.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Perez-Carrillo et al. - 2016 - Geolocation Adaptive Music Player.pdf:pdf},
type      = {Paper},
url       = {https://smartech.gatech.edu/bitstream/handle/1853/54586/geolocation_videostream.html?sequence=8&isAllowed=y},
}

@InProceedings{2016_EA_16,
author    = {Feenstra, Evan},
title     = {BeatPush},
booktitle = {Proceedings of the International Web Audio Conference},
year      = {2016},
editor    = {Freeman, Jason and Lerch, Alexander and Paradis, Matthew},
number    = {530},
series    = {WAC '16},
pages     = {4919},
address   = {Atlanta, Georgia},
month     = apr,
publisher = {Georgia Tech},
abstract  = {BeatPush is a web-based, mobile-ready music production environment. BeatPush's modular layout is designed to be intuitive for students and music production beginners. Audio samples and synthesized notes are displayed as colors, while sequences are arranged in a ring, to represent the cyclical nature of music. Features such as auto-harmonization, drag-and-drop sequencing, and one-click effect automation make it easy to create complex electronic music. BeatPush interfaces with most of the Web Audio API processing nodes, including waveform synthesis, variable speed sample playback, filters, convolution (reverb), compression, envelopes, delay, distortion, panning, etc. BeatPush also integrates the Freesound.org API, so users can explore millions of Creative Commons licensed audio samples, and import them into their projects. Songs created with BeatPush can be saved, distributed across devices, and shared with friends on social media.},
file      = {:Users/Hjem/Library/Application Support/Mendeley Desktop/Downloaded/Feenstra - 2016 - BeatPush.pdf:pdf},
type      = {Demo},
}