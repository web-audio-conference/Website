[{"Media":"https://youtu.be/GigSl1yGmII","Paper ID":"2","Paper Title":"Challenges in the Development of an Easy-Access Mobile Phone Orchestra Platform","Abstract":"How can an easy-access Mobile Phone Orchestra (MPO) platform be developed to facilitate high-end, flexible artistic expressions suitable for concert hall performances? Moreover, what challenges and possibilities arise in the development process of a novel MPO platform embracing school children as performers? In this paper, we discuss potential answers to these questions, which were integrated in the development process of mobilephoneorchestra.com. In particular, we highlight the main challenge of developing a performance platform that is both mobile and artistically rewarding, but still easily accessed by schoolchildren. Mobilephoneorchestra.com was developed as a Web Audio API to enable large-scale concert hall performances of fixed polyphonic contemporary art music. It is a kind of philharmonic orchestra for electronic sounds that embraces school children as performers and uses smart phones as instruments. Through an autoethnographic approach, our research was carried out in multiple iterations, inspired by action research. In this paper, we present findings from three iterations that include performances of music involving MPO. Schoolchildren/youths between 10 and 17 years old were addressed as MPO performers. The findings reveal both the artistic challenges of the platform and the possibilities. We specifically highlight the use of mobile music interfaces in combination with animated notation as a novel approach for an MPO concept.","Author Names":"Anders Lind (Sounds like Lind)*; Björn Yttergren (Umeå); Håkan  Gustafson (Studio With)","Author Emails":"lind@soundslikelind.se; bjorn.yttergren@gmail.com; hakan@studiowith.se","Primary Contact Author Email":"lind@soundslikelind.se","Track Name":"Paper","Files":"MPO_LIND_Camera ready.pdf (783460 bytes)","Pages":""},{"Media":"https://youtu.be/HDijiua370I","Paper ID":"4","Paper Title":"Build WebAudio and JavaScript Web Applications using JSPatcher: A Web-based Visual Programming Editor","Abstract":"Many visual programming languages (VPLs) such as Max [1] or PureData [2] provide a graphic canvas to allow developers to connect functions or data between them. This canvas, also known as a patcher [3], is basically a graph meant to be interpreted as dataflow computation by the system. Some VPLs are used for multimedia performance or content generation as the UI system is often an important part of the language. This paper presents a web-based VPL, JSPatcher, which allows not only to build audio graphs using the WebAudio API, but also to design graphically AudioWorklet DSPs with FAUST toolchain, [4] [5] or to create interactive programs with other language built-ins, Web APIs or any JavaScript modules.","Author Names":"Shihong Ren (Université Jean Monnet)*; Laurent Pottier (CIEREC); Michel Buffa (Université Côte d’Azur, CNRS, INRIA, FRANCE)","Author Emails":"renshihong@hotmail.com; laurent.pottier@univ-st-etienne.fr; michel.buffa@univ-cotedazur.fr","Primary Contact Author Email":"renshihong@hotmail.com","Track Name":"Paper","Files":"WAC-21-lite.pdf (580215 bytes)","Pages":""},{"Media":"https://youtu.be/h6SH8QD5bSk","Paper ID":"5","Paper Title":"A Front End for Adaptive Online Listening Tests","Abstract":"A number of tools to create online listening tests are currently available. They provide an integrated platform consisting of a user-facing front end and a back end to collect responses. These platforms provide an out-of-the-box solution for setting up static listening tests, where questions and audio stimuli remain unchanged and user-independent. In this paper, we detail the changes we made to the webMUSHRA platform to convert it into a front end for adaptive online listening tests. Some of the more advanced workflows that can be built around this front end include session management to resume listening tests, server-based sampling of stimuli to enforce a certain distribution over all participants, and follow-up questions based on previous responses. The back ends required for such workflows need a large amount of customisation based on the exact listening test specification, and are therefore deemed out of scope for this project. Consequently, the proposed front end is not meant as a replacement for the existing webMUSHRA platform, but as starting point to create custom listening tests. Nonetheless, a fair number of the proposed changes are also beneficial for the creation of static listening tests.","Author Names":"Johan Pauwels (Imperial College London)*; Simon Dixon (Queen Mary University of London); Joshua D. Reiss (Queen Mary University of London)","Author Emails":"j.pauwels@qmul.ac.uk; s.e.dixon@qmul.ac.uk; joshua.reiss@qmul.ac.uk","Primary Contact Author Email":"j.pauwels@qmul.ac.uk","Track Name":"Paper","Files":"pauwels2021wac-cameraready.pdf (434960 bytes)","Pages":""},{"Media":"https://youtu.be/YgY2stL5lt0","Paper ID":"8","Paper Title":"Glicol: A Graph-oriented Live Coding Language Developed with Rust, WebAssembly and AudioWorklet","Abstract":"This paper introduces the new music live coding language Glicol (graph-oriented live coding language) and its web-based run-time environment. As the name suggests, this language is designed to represent directed acyclic graphs (DAG), using a syntax optimised for live music performances. The audio engine and the language interpreter are both developed with the Rust programming language. With the help of WebAssembly and AudioWorklet, this language can run in web browsers. It also enables co-performance with the support for collaborative editing. Taking advantages of the Rust programming language design, the run-time environment is both safe and efficient. Documentation and error handling messages can be accessed in the web browser. All in all, we see Glicol as an efficient and future-oriented language for collaborative text-based musicking.","Author Names":"Qichao Lan (University of Oslo)*; Alexander Refsum Jensenius (University of Oslo)","Author Emails":"qichao.lan@imv.uio.no; a.r.jensenius@imv.uio.no","Primary Contact Author Email":"qichao.lan@imv.uio.no","Track Name":"Paper","Files":"Glicol_WAC_paper_final (27).pdf (1460687 bytes)","Pages":""},{"Media":"https://youtu.be/Sd71US-sU-A","Paper ID":"10","Paper Title":"Putting Web Audio API to the test: Introducing WebAudioXML as a pedagogical platform","Abstract":"Web technologies in general and Web Audio API in particular have a great potential as a learning platform for developing interactive sound and music applications. Earlier studies at the Royal College of Music in Stockholm have led to a wide range of student projects but have also indicated that there is a high threshold for novice programmers to understand and use Web Audio API. We developed the WebAudioXML coding environment to solve this problem, and added a statistics module to analyze student works. Three groups of students with technical respectively artistic background participated through online courses by building interactive, sound-based applications. We analysed the projects to understand the impact WebAudioXML has on creativity and the learning process. The results indicate that WebAudioXML can be a useful platform for teaching and learning how to build online audio applications. The platform makes mapping between user interactions and audio parameters accessible for novice programmer and supports artists in successfully realizing their design ideas. We show that templates can be a great help for the students to get started but also a limitation for them to expand ideas beyond the presented scope.","Author Names":"Hans Lindetorp (KMH)*; Kjetil Falkenberg (KTH Sound and Music Computing group)","Author Emails":"hans.lindetorp@kmh.se; kjetil@kth.se","Primary Contact Author Email":"hans.lindetorp@kmh.se","Track Name":"Paper","Files":"Lindetorp-Falkenberg - Putting Web Audio API to the test.pdf (210186 bytes)","Pages":""},{"Media":"https://youtu.be/yYZp7zXnyFk","Paper ID":"13","Paper Title":"immaterial.cloud: Using peer-to-peer technologies for music","Abstract":"immaterial.cloud is an immersive audiovisual installation that explores a possible networked future of peer-to-peer technologies, away from cloud computing. Participants experience the work via two to four smartphones placed in\ndifferent locations in a room. As participants walk up to a phone, they see a representation of themselves through data. If the participant gets close enough, the phone triggers a change in the sound of immaterial.cloud and the other phones follow.","Author Names":"Tate Carson (Louisiana State University)*","Author Emails":"tatecarson@gmail.com","Primary Contact Author Email":"tatecarson@gmail.com","Track Name":"Paper","Files":"Carson_immaterial_cloud_CameraRead.pdf (1555026 bytes)","Pages":""},{"Media":"https://youtu.be/7HBBV78qoNQ","Paper ID":"20","Paper Title":"JackTrip-WebRTC: Networked music experiments with PCM stereo audio in a Web browser","Abstract":"A large number of web applications are available for videoconferencing and those have been very helpful during the lockdown periods caused by the COVID-19 pandemic. However, none of these offer high fidelity stereo audio for music performance, mainly because the current WebRTC RTCPeerConnection standard only supports compressed audio formats. This paper presents the results achieved implementing 16-bit PCM stereo audio transmission on top of the WebRTC RTCDataChannel with the help of Web Audio and AudioWorklets.\nSeveral measurements with different configurations, browsers, and operating systems are presented. They show that, at least on the loopback network interface, this approach can achieve better quality and lower latency than using RTCPeerConnection, i.e., latencies as low as 50-60~ms have been achieved on MacOS.","Author Names":"Matteo Sacchetto (Politecnico di Torino); Antonio Servetti (Politecnico di Torino)*; Chris Chafe (organization)","Author Emails":"matteo.sacchetto@polito.it; antonio.servetti@polito.it; cc@ccrma.stanford.edu","Primary Contact Author Email":"antonio.servetti@polito.it","Track Name":"Paper","Files":"Jacktrip_WebRTC_WAC21 (12).pdf (721564 bytes)","Pages":""},{"Media":"https://youtu.be/JXctQvUXJPA","Paper ID":"21","Paper Title":"Improvisation in Isolation: Quarentena Liv(r)e and Noise Symphony with the Playsound online music making tool","Abstract":"In this paper, we describe artistic practices with the web-based music making tool Playsound.space held over the last two years since the inception of the platform. After completing a first design phase, which is documented in previous publications, Playsound has been regularly used by the first author as her main musical instrument to perform in live improvisation contexts. The tool proved to be especially useful during the quarantine period due to Covid-19 in Brazil, by enabling the musician (i) to take part in performances with other musicians through online gatherings, and (ii) to compose solo pieces from home leveraging crowd-sourced sounds. We review these endeavours and provide a critical analysis exploring some of the benefits of online music making and related challenges yet to tackle. The use of Playsound \"in vivo\", in real artistic practices outside the laboratory, has enabled us to uncover playing strategies and user interface improvements which can inform the design of similar web-based music making tools.","Author Names":"Ariane S Stolfi (Universidade Federal do Sul da Bahia)*; Diogo  Pereira (Instituto Federal de Educação, Ciência e Tecnologia da Bahia); Mathieu Barthet (QMUL)","Author Emails":"arianestolfi@gmail.com; diogopereira@ifba.edu.br; m.barthet@qmul.ac.uk","Primary Contact Author Email":"arianestolfi@gmail.com","Track Name":"Paper","Files":"Noise_Symphony_and_Quarentena_Livre_06_compressed.pdf (278915 bytes)","Pages":""},{"Media":"https://youtu.be/NxkrdQrobGs","Paper ID":"25","Paper Title":"CosmoNote: A Web-based Citizen Science Tool for Annotating Music Performances","Abstract":"CosmoNote is a web-based citizen science tool for annotating musical structures, with a focus on structures created by the performer during expressive musical performance. The software interface enables the superimposition of synchronized discrete and continuous information layers which include note data representations, audio features such as loudness and tempo, and score features such as harmonic tension in a visual and audio environment. The tools provide the means for users to signal performance decisions such as segmentation and prominence using boundaries of varying strengths, regions, comments, and note groupings. User-friendly interaction features have been built in to facilitate ease of annotation; these include the ability to zoom in, listen to, and mark up specific segments of music. The data collected will be used to discover the vocabulary of performed music structures and to aid in the understanding of expressive choices and nuances.","Author Names":"Lawrence Fyfe (CNRS-UMR9912/STMS IRCAM, Paris, France)*; Daniel Bedoya (CNRS-UMR9912/STMS IRCAM, Paris, France); Corentin Guichaoua (CNRS-UMR9912/STMS IRCAM, Paris, France); Elaine Chew (CNRS-UMR9912/STMS (IRCAM), Paris, France)","Author Emails":"lawrence.fyfe@ircam.fr; daniel.bedoya@ircam.fr; corentin.guichaoua@ircam.fr; elaine.chew@ircam.fr","Primary Contact Author Email":"lawrence.fyfe@ircam.fr","Track Name":"Paper","Files":"CosmoNote.pdf (2926944 bytes)","Pages":""},{"Media":"https://youtu.be/kNdcm5cVZk8","Paper ID":"27","Paper Title":"Choir Singers Pilot – An online platform for choir singers practice","Abstract":"We present the Choir Singers Pilot, a web-based system that assists choir singers in their individual learning and practice. Our system is built using modern web technologies and provides singers with an interactive view of the musical score along with an aligned audio performance created using state-of-the-art singing synthesis technology. The Web Audio API is used to dynamically mix the choir voices and give users control over sound parameters. In-browser audio latency compensation is used to keep user recordings aligned to the reference music tracks. The pitch is automatically extracted from user recordings and can then be analyzed using an assessment algorithm to provide intonation ratings to the user. The system also facilitates communication and collaboration in choirs by enabling singers to share their recordings with conductors and receive feedback. Our work, in the larger scope of the TROMPA project, aims to enrich musical activities and promote use of digital music resources. To that end, we also synthesize thousands of public-domain choir scores and make them available in a searchable repository alongside relevant metadata for public consumption.","Author Names":"Matan Gover (Voctro Labs)*; Álvaro Sarasúa (Voctro Labs); Hector Parra (Voctro Labs); Jordi Janer (Voctro Labs); Oscar Mayor (Voctro Labs); Helena Cuesta (Universitat Pompeu Fabra); Maria Pilar Pascual (Universitat Pompeu Fabra); Aggelos Gkiokas (Universitat Pompeu Fabra); Emilia Gomez (Universitat Pompeu Fabra)","Author Emails":"matangover@gmail.com; alvarosarasua@gmail.com; hector@parra.cat; jordi.janer@voctrolabs.com; oscar.mayor@voctrolabs.com; helena.cuesta@upf.edu; mariapilar.pascual@upf.edu; aggelos.gkiokas@upf.edu; emilia.gomez@upf.edu","Primary Contact Author Email":"matangover@gmail.com","Track Name":"Paper","Files":"CSP Paper - Camera Ready.pdf (734162 bytes)","Pages":""},{"Media":"https://youtu.be/hyPoX7PMKs8","Paper ID":"30","Paper Title":"Online Audio Editor in Freesound","Abstract":"Freesound has a huge community dedicated to upload and share sounds online for other users to use those sounds in their productions. In some cases, the sounds will need to be transformed and users will have to go in their devices and edit that sound in order to get the exact version that fits their production the most. In this paper, an audio editor is developed and added into the Freesoud environment in order to have those two steps together in the same workflow. This approach will add a huge value to the Freesound experience and will differentiate it from its competitors. For the development of this editor, different Javascipt and HTML frameworks have been used, like Wavesurfer (based on Web Audio API) or Bootstrap, always trying to keep the interface and functionalities accessible and easy-to-use to a general audience. Finally, two evaluation methods have been developed, the first in order to get some general feedback about the use of the editor and the second one to be sure that all its different functionalities behave as expected. The first evaluation showed the need of a simpler interface and functionalities, which was developed and then tested using the second evaluation approach, showing that all different actions, filters and effects worked as expected.","Author Names":"Roberto Pérez Sánchez (Universidad Pompeu Fabra)*; Frederic Font (Music Technology Group - Universitat Pompeu Fabra)","Author Emails":"robertops1818@gmail.com; frederic.font@upf.edu","Primary Contact Author Email":"robertops1818@gmail.com","Track Name":"Paper","Files":"Online Audio Editor - WAC - Camera Ready.pdf (1281929 bytes)","Pages":""},{"Media":"https://youtu.be/CBaFsSVrmrk","Paper ID":"31","Paper Title":"Play the place: Experiencing architectural spaces using a web-based 3D audio environment","Abstract":"Assessing the acoustics of an architectural space before its physical construction is of interest to architects and structural engineers. In this work, we present a browser-based interactive app enabling sonic interaction in a virtual environment modeled after a small studio architectural design for music ideation and creation made by architects Stephan & Eric Zimmerli, called ‘Studiolo’. We first describe our ideation and design process involving pilot testing in the New Atlantis online environment for a collaborative sound and music experience. We then describe the implementation of the ‘Studiolo’ web app prototype supported by Javascript packages (Tone, Resonance Audio, Three, and Tween) and the Web Audio API. Room acoustics is modeled using the EVERTims framework providing a real-time auralization engine for binaural room impulse response approximation. A critical analysis by one of the architects highlights the importance of multimodal factors, from visual mapping textures and lighting quality to more sensitive and responsive timbres addressing the “tactile dimension” of music creation.","Author Names":"Clifford M Manasseh (Queen Mary University of London)*; Mathieu Barthet (QMUL); Stephan Zimmerli (Architect)","Author Emails":"ec19461@qmul.ac.uk; m.barthet@qmul.ac.uk; stephan.zimmerli@gmail.com","Primary Contact Author Email":"ec19461@qmul.ac.uk","Track Name":"Paper","Files":"Studiolo_WAC_2021.pdf (2555284 bytes)","Pages":""},{"Media":"https://youtu.be/Y7B6CKiVPC0","Paper ID":"35","Paper Title":"Collaborative Music Creation and Performance with Soundcool Online","Abstract":"Soundcool Online is a Web Audio re-implementation of the original Max/MSP implementation of Soundcool, a system for collaborative music and audiovisual creation. Soundcool has many educational applications, and because Linux has been adopted in many school systems, we turned to Web Audio to enable Soundcool to run on Linux as well as many other platforms. An additional advantage of Soundcool Online is the elimination of a large download, allowing beginners to try the system more easily. Another advantage is the support for sharing provided by a centralized server, where projects can be stored and accessed by others. A cloud-based server also facilitates collaboration at a distance where multiple users can control the same project. In this scenario, local sound synthesis provides high-quality sound without the large bandwidth requirements of shared audio streams. Experience with Web Audio and latency measurements are reported.","Author Names":"Roger Dannenberg (School of Computer Science, Carnegie Mellon University)*; Huan Zhang (Carnegie Mellon University); Amit Meena (Indian Institute of Technology); Ankitkumar Joshi (University of Pittsburgh); Amey Patel (Indian Institute of Technology); Jorge Sastre (Universitat Politecnica de Valencia)","Author Emails":"rbd@cs.cmu.edu; huanz@andrew.cmu.edu; amitmeena094@gmail.com; ahjoshi@pitt.edu; ameykpatel@gmail.com; jsastrem@upv.es","Primary Contact Author Email":"rbd@cs.cmu.edu","Track Name":"Paper","Files":"Web_Soundcool.pdf (1447546 bytes)","Pages":""},{"Media":"https://youtu.be/z4r8DmioEAo","Paper ID":"36","Paper Title":"Essentia TensorFlow Models for Audio and Music Processing on the Web","Abstract":"Recent advances in web-based machine learning (ML) tools empower a wide range of application developers in both industrial and creative contexts. The availability of pre-trained ML models and JavaScript (JS) APIs in frameworks like TensorFlow.js enabled developers to use AI technologies without demanding domain expertise. Nevertheless, there is a lack of pre-trained models in web audio compared to other domains, such as text and image analysis. Motivated by this, we present a collection of open pre-trained TensorFlow.js models for music-related tasks on the Web. Our models currently allow for different types of music classification (e.g., genres, moods, danceability, voice or instrumentation), tempo estimation, and music feature embeddings. To facilitate their use, we provide a dedicated JS add-on module essentia.js-model within the Essentia.js library for audio and music analysis. It has a simple API, enabling end-to-end analysis from audio input to prediction results on web browsers and Node.js. Along with the Web Audio API and web workers, it can also be used to build real-time applications. We provide usage examples, discuss possible use-cases, and report benchmarking results.","Author Names":"Albin Correya (Universitat Pompeu Fabra); Pablo Alonso-Jiménez (Universitat Pompeu Fabra); Jorge Marcos-Fernández (Universitat Pompeu Fabra); Xavier Serra (Universitat Pompeu Fabra); Dmitry Bogdanov (Universitat Pompeu Fabra)*","Author Emails":"albin.correya@upf.edu; pablo.alonso@upf.edu; jorge.marcos@upf.edu; xavier.serra@upf.edu; dmitry.bogdanov@upf.edu","Primary Contact Author Email":"dmitry.bogdanov@upf.edu","Track Name":"Paper","Files":"WAC_2020_essentia_js_models.pdf (558032 bytes)","Pages":""},{"Media":"https://youtu.be/ezpct-d5_kE","Paper ID":"43","Paper Title":"A Time-Travel Debugger for Web Audio Applications","Abstract":"Developing real-time audio applications, particularly those with an element of user interaction, can be a difficult task. When things go wrong, it can be challenging to locate the source of a problem when many parts of the program are connected and interacting with one another in real-time.\n\nWe present a time-travel debugger for the Flow Web Audio framework that allows developers to record a session interacting with their program, playback that session with the original timing still intact, and step through individual events to inspect the program state at any point in time. In contrast to the browser's native debugging features, audio processing remains active while the time-travel debugger is enabled, allowing developers to listen out for audio bugs or unexpected behaviour.\n\nWe describe three example use-cases for such a debugger. The first is error reproduction using the debugger's JSON import/export capabilities to ensure the developer can replicate problematic sessions. The second is using the debugger as an exploratory aid instead of a tool for error finding. Finally, we consider opportunities for the debugger's technology to be used by end-users as a means of recording, sharing, and remixing ideas. We conclude with some options for future development, including expanding the debugger's program state inspector to allow for in situ data updates, visualisation of the current audio graph similar to existing Web Audio inspectors, and possible methods of evaluating the debugger's effectiveness in the scenarios described.","Author Names":"Andrew Thompson (Queen Mary University of London)*; Gyorgy Fazekas (Queen Mary University of London); Geraint A. Wiggins (Vrije Universiteit Brussel)","Author Emails":"andrew.thompson@qmul.ac.uk; g.fazekas@qmul.ac.uk; geraint@ai.vub.ac.be","Primary Contact Author Email":"andrew.thompson@qmul.ac.uk","Track Name":"Paper","Files":"thompson2021debugger.pdf (302059 bytes)","Pages":""},{"Media":"https://youtu.be/neUBKQzs9HQ","Paper ID":"47","Paper Title":"Surfing with Narcissus: Updating the Technology used in Thea Musgrave’s work for Solo Clarinet","Abstract":"Narcissus is a work created in 1987 by Thea Musgrave for clarinet (or flute) and delayed audio that uses a delay box. In order to recreate the piece using modern technology, our research team developed a web application to emulate the functions of the original equipment in Narcissus. The present paper is our report about the experience of recreating Musgrave’s work using web audio and web music APIs.","Author Names":"Luan Luiz Gonçalves (Federal University of São João del-Rei (UFSJ))*; Rafael Andrade (Federal University of São João del-Rei (UFSJ)); Iura Sobrinho (Federal University of São João del-Rei (UFSJ)); Flávio Luiz Schiavoni (Federal University of São João del-Rei)","Author Emails":"human@flossuniverse.org; rafael.asa@outlook.com; iuraderezende@gmail.com; fls@ufsj.edu.br","Primary Contact Author Email":"human@flossuniverse.org","Track Name":"Paper","Files":"WAC2021_Narcissus (3).pdf (502764 bytes)","Pages":""},{"Media":"https://youtu.be/Lgv9pitgusc","Paper ID":"48","Paper Title":"Composing and Improvising Using Sound Content-Based Descriptive Filtering","Abstract":"The Freesound Player is a digital instrument developed in Max MSP that uses the Freesound API to make requests for as many as 16 sound samples which are filtered based on sound content. Once the samples are returned they are loaded into buffers and can be performed using a MIDI controller and processed in a variety of ways. The filters implemented will be discussed and demonstrated using three music compositions by the authors, along with considerations for composing and improvising using sound content-based descriptive filtering.","Author Names":"Dylan Burchett (Louisiana State University); William Thompson (Louisiana State University); Austin A Franklin (Louisiana State University)*","Author Emails":"dburch6@lsu.edu; wthom53@lsu.edu; austinalexanderfranklin12@gmail.com","Primary Contact Author Email":"austinalexanderfranklin12@gmail.com","Track Name":"Paper","Files":"Composing_and_improvising_Using_Sound_Content_Based_Descriptive_Filtering.pdf (2651513 bytes)","Pages":""},{"Media":"https://youtu.be/0QaM5xgTfEM","Paper ID":"49","Paper Title":"kilobeat: low-level collaborative livecoding","Abstract":"This paper presents kilobeat, a collaborative, web-based, DSP-oriented livecoding platform. Players make music together by writing short snippets of code. Inspired by the practices of bytebeat, these snippets are low-level expressions representing digital audio signals. Unlike existing platforms, kilobeat does not adapt existing livecoding languages or introduce a new one: players write JavaScript expressions (using standard operators and math functions) to generate samples directly. This approach reduces the amount of background knowledge required to understand players' code and makes kilobeat amenable to synthesis and DSP pedagogy. To facilitate collaboration, players can hear each other's audio (distributed spatially in a virtual room), see each other's code (including edits and run actions), and depend on each other's output (by referencing other players as variables). Additionally, performances may be recorded and replayed later, including all player actions. For accessibility and ease of sharing, kilobeat is built on Web Audio and WebSockets.","Author Names":"Ian J Clester (Georgia Institute of Technology)*","Author Emails":"ijc@gatech.edu","Primary Contact Author Email":"ijc@gatech.edu","Track Name":"Paper","Files":"kilobeat_final.pdf (312061 bytes)","Pages":""},{"Media":"https://youtu.be/xHMEmBZNwWc","Paper ID":"55","Paper Title":"Functional Reactive Programming and the Web Audio API","Abstract":"Functional Reactive Programming (FRP) is a way to model temporal phenomena using events, which carry information corresponding to a precise moment in time, and behaviors, which represent time-varying values. This paper shows how FRP can be used to build reactive audio applications that blend the WebAudio API with other browser-based APIs, such as mouse events and MIDI events.  It will start by presenting a brief history of FRP as well as definitions of the Event and Behavior types.  It will then discuss the principal challenges of applying the behavior pattern to WebAudio and how these challenges can be solved by using induction on existentially-quantified and linearly-typed Indexed Cofree Comonads. An implementation of this approach is provided via the library purescript-wags.","Author Names":"Mike Solomon (Meeshkan)*","Author Emails":"mike@meeshkan.com","Primary Contact Author Email":"mike@meeshkan.com","Track Name":"Paper","Files":"sig-alternate-sample.pdf (156084 bytes)","Pages":""}]